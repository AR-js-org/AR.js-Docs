{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AR.js - Augmented Reality on the Web AR.js is a lightweight library for Augmented Reality on the Web, coming with features like Image Tracking, Location based AR and Marker tracking. What Web AR means (Augmented Reality on the Web) Augmented Reality is the technology that makes possible to add overlayed content on the real world. It can be provided for several type of devices: handleheld (like mobile phones), headsets, desktop displays, and so on. For handleheld devices (more in general, for video-see-through devices) the 'reality' is captured from one or more cameras and then shown on the device display, adding some kind of content on top of it. For developers, to develop Augmented Reality ('AR' from now on) on the Web, means to void all the Mobile app developement efforts and costs related to App stores (validation, time to publish). It also means to re-use well known technologies like Javascript, HTML and CSS, known from a lot of developers and possibly designers. It basically means that is possible to release every new version instantly, fix bugs or release new features in near real-time, opening a lot of pratical possibilities. For users, it means to reach an AR experience just visiting a website. As QR Codes are now widespread, it's also possible to scan a QR Code and reach the URL without even type. Addictionally, users do not have to reserve storage space on their download the AR app, and do not have to keep it updated. Why AR.js We believe in the Web, as a collaborative and accessible environment. We also believe in the Augmented Reality technology, as a new communication medium, that can help people to see the reality in new, exciting ways. We see Augmented Reality (AR) used everyday for a lot of useful applications, from art, to education, also for fun. We strongly believe that such a powerful technology, that can help people and leverage their creativity, should be free in some way. Also collaborative, if possible. And so, we continue the work started by Jerome Etienne, in bringing AR on the Web, as a free and Open Source technology. Thank you for being interested in this, if you'd like to collaborate in any way, contact us ( https://twitter.com/nicolocarp ). The project is now under a Github organization, that you can find at https://github.com/ar-js-org and you can ask to be part of it, for free. AR types AR.js features the following types of Augmented Reality, on the Web: Image Tracking , when a 2D images is found by the camera, it's possible to show some kind of content on top of it, or near it. The content can be a 2D image, a GIF, a 3D model (also animated) and a 2D video too. Cases of use: Augmented Art, learning (Augmented books), Augmented flyers, advertising, etc. Location Based AR , this kind of AR uses real-world places in order to show Augmented Reality content, on the user device. The experiences that can be built with this library are those that uses users position in the real world. The user can move (ideally outdoor) and through their smartphones they can see AR content where places are in the real world. Moving around and rotating the phone will make the AR content change according to users position and rotation (so places are 'sticked' in their real position, and appear bigger/thinner according to their distance from the user). With this solution it\u2019s possible to build experiences like interactive support for touristic guides, support when exploring a new city, find places of interest like buildings, museums, restaurants, hotels and so on. It\u2019s also possible to build learning experiences like treasure hunts and biology or history learning games, or use this technology for situated art (visual art experiences bound to specific real world coordinates). Marker Tracking , When a marker is found by the camera, it's possible to show some content (same as Image Tracking). Markers are very stable but limited in shape, color and size. It is suggested for those experiences where are required a lot of different markers with different content. Examples of use: (Augmented books), Augmented flyers, advertising. Key points Very Fast : It runs efficiently even on phones Web-based : It is a pure web solution, so no installation required. Full javascript based on three.js + A-Frame + jsartoolkit5 Open Source : It is completely open source and free of charge! Standards : It works on any phone with webgl and webrtc AR.js has reached version 3. This is the official repository: https://github.com/AR-js-org/AR.js . If you want to visit the old AR.js repository, here it is: https://github.com/jeromeetienne/AR.js . Import the library AR.js from version 3 has a new structure. AR.js is coming in two, different build. They are both maintained. They are exclusive. The file you want to import depends on what features you want, and also which render library you want to use (A-Frame or three.js). AR.js uses jsartoolkit5 for tracking, but can display augmented content with either three.js or A-Frame . You can import AR.js in one version of your choice, using the script tag on your HTML. AR.js with Image Tracking + Location Based AR Import AFRAME version: script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js Import three.js version: script src= https://raw.githack.com/AR-js-org/AR.js/master/three.js/build/ar-nft.js AR.js with Marker Tracking + Location Based AR: Import AFRAME version: script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js Import three.js version: script src= https://raw.githack.com/AR-js-org/AR.js/master/three.js/build/ar.js If you want to import a specific version, you can do that easily replacing master with the version tag, e.g.: script src= https://raw.githack.com/AR-js-org/AR.js/3.0.0/aframe/build/aframe-ar-nft.js Requirements Some requirements and known restrictions are listed below: It works on every phone with webgl and webrtc . Marker based is very lightweight, while Image Tracking is more CPU consuming You cannot use Chrome on iOS, as Chrome on iOS did not support, at the moment, camera access On device with multi-cameras, Chrome may have problems on detecting the right one. Please use Firefox if you find that AR.js opens on the wrong camera. There is an open issue for this. To work with Location Based feature, your phone needs to have GPS sensors Please, read carefully any suggestions that AR.js pops-up -as alerts- for Location Based on iOS, as iOS requires user actions to activate geoposition Location Based feature is only available on A-Frame Always deploy under https Accessing to the phone camera or to camera GPS sensors, due to major browsers restrictions, can be done only under https websites. All the examples you will see, and all AR.js web apps in general, have to be run on a server. You can use local server or deploy the static web app on the web. So don't forget to always run your examples on secure connections servers or localhost. Github Pages is a great way to have free and live websites under https. Getting started Here we present three, basic examples, one for each AR feature. For specific documentation, on the top menu you can find every section, or you can click on the following links: Image Tracking Documentation Location Based Documentation Marker Based Documentation Image Tracking Example There is a Codepen for you to try. Below you can find also a live example. Please follow these simple steps: Create a new project with the code below (or open this live example and go directly to the last step) Run it on a server Open the website on your phone Scan this picture to see content through the camera. script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded -- div class= arjs-loader div Loading, please wait... /div /div a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- we use cors proxy to avoid cross-origin problems -- a-nft type= nft url= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/trex-image/trex smooth= true smoothCount= 10 smoothTolerance= .01 smoothThreshold= 5 a-entity gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf scale= 5 5 5 position= 50 150 0 /a-entity /a-nft a-entity camera /a-entity /a-scene /body Location Based Example Try it live with this Codepen . It retrieves your position and places a text near you. Please follow these simple steps: Create a new project with the following snippet, and change add-your-latitude and add-your-longitude with your latitude and longitude, without the . Run it on a server Activate GPS on your phone and navigate to the example URL Look around. You should see the text looking at you, appearing in the requested position, even if you look around and move the phone. !DOCTYPE html html head meta charset= utf-8 / meta http-equiv= X-UA-Compatible content= IE=edge / title GeoAR.js demo /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script /head body style= margin: 0; overflow: hidden; a-scene vr-mode-ui= enabled: false embedded arjs= sourceType: webcam; debugUIEnabled: false; a-text value= This content will always face you. look-at= [gps-camera] scale= 120 120 120 gps-entity-place= latitude: add-your-latitude ; longitude: add-your-longitude ; /a-text a-camera gps-camera rotation-reader /a-camera /a-scene /body /html If you want to enhance and customize your Location Based experience, take a look at the Location Based docs. Marker Based Example Please follow these simple steps: Create a new project with the code below (or open this live example and go directly to the last step) Run it on a server Open the website on your phone Scan this picture to see content through the camera. !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script body style= margin : 0px; overflow: hidden; a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html Advanced stuff AR.js offers two ways, with A-Frame, to interact with the web page: to interact directly with AR content and Overlayed DOM interaction. Also, there are several Custom Events triggered during the life cycle of every AR.js web app. You can learn more about these aspects on the UI and Events section . AR.js architecture AR.js uses jsartoolkit5 for tracking, but can display augmented content with either three.js or A-Frame . three.js folder contains source code for AR.js core, Marker based and Image Tracking examples for AR.js three.js based build for three.js AR.js based vendor stuff (jsartoolkit5) workers (used for Image Tracking). When you find files that ends with -nft suffix, they're boundled only with the Image Tracking version. A-Frame version of AR.js uses three.js parts as its core. A-Frame code, on AR.js, is simply a wrapper to write AR with Custom Components in HTML. aframe folder contains source code for AR.js A-Frame (aka wrappers for Marker Based, Image Tracking components) source code for Location Based build for A-Frame AR.js based examples for A-Frame AR.js. Tutorials There are various tutorials available for developing with AR.js. These include: Location Based Build your Location-Based Augmented Reality Web App Develop a Simple Peakfinder App ( Provided with these docs ) Troubleshooting, feature requests, community You can find a lot of help on the old AR.js repositories issues . Please search on open/closed issues, you may find a interesting stuff. Contributing From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain. Issues If you are having configuration or setup problems, please post a question to StackOverflow . You can also address question to us in our Gitter chatroom If you have discovered a bug or have a feature suggestion, feel free to create an issue on Github. Submitting Changes After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Some things that will increase the chance that your pull request is accepted: Follow the existing coding style Write a good commit message","title":"Home"},{"location":"#arjs-augmented-reality-on-the-web","text":"AR.js is a lightweight library for Augmented Reality on the Web, coming with features like Image Tracking, Location based AR and Marker tracking.","title":"AR.js - Augmented Reality on the Web"},{"location":"#what-web-ar-means-augmented-reality-on-the-web","text":"Augmented Reality is the technology that makes possible to add overlayed content on the real world. It can be provided for several type of devices: handleheld (like mobile phones), headsets, desktop displays, and so on. For handleheld devices (more in general, for video-see-through devices) the 'reality' is captured from one or more cameras and then shown on the device display, adding some kind of content on top of it. For developers, to develop Augmented Reality ('AR' from now on) on the Web, means to void all the Mobile app developement efforts and costs related to App stores (validation, time to publish). It also means to re-use well known technologies like Javascript, HTML and CSS, known from a lot of developers and possibly designers. It basically means that is possible to release every new version instantly, fix bugs or release new features in near real-time, opening a lot of pratical possibilities. For users, it means to reach an AR experience just visiting a website. As QR Codes are now widespread, it's also possible to scan a QR Code and reach the URL without even type. Addictionally, users do not have to reserve storage space on their download the AR app, and do not have to keep it updated.","title":"What Web AR means (Augmented Reality on the Web)"},{"location":"#why-arjs","text":"We believe in the Web, as a collaborative and accessible environment. We also believe in the Augmented Reality technology, as a new communication medium, that can help people to see the reality in new, exciting ways. We see Augmented Reality (AR) used everyday for a lot of useful applications, from art, to education, also for fun. We strongly believe that such a powerful technology, that can help people and leverage their creativity, should be free in some way. Also collaborative, if possible. And so, we continue the work started by Jerome Etienne, in bringing AR on the Web, as a free and Open Source technology. Thank you for being interested in this, if you'd like to collaborate in any way, contact us ( https://twitter.com/nicolocarp ). The project is now under a Github organization, that you can find at https://github.com/ar-js-org and you can ask to be part of it, for free.","title":"Why AR.js"},{"location":"#ar-types","text":"AR.js features the following types of Augmented Reality, on the Web: Image Tracking , when a 2D images is found by the camera, it's possible to show some kind of content on top of it, or near it. The content can be a 2D image, a GIF, a 3D model (also animated) and a 2D video too. Cases of use: Augmented Art, learning (Augmented books), Augmented flyers, advertising, etc. Location Based AR , this kind of AR uses real-world places in order to show Augmented Reality content, on the user device. The experiences that can be built with this library are those that uses users position in the real world. The user can move (ideally outdoor) and through their smartphones they can see AR content where places are in the real world. Moving around and rotating the phone will make the AR content change according to users position and rotation (so places are 'sticked' in their real position, and appear bigger/thinner according to their distance from the user). With this solution it\u2019s possible to build experiences like interactive support for touristic guides, support when exploring a new city, find places of interest like buildings, museums, restaurants, hotels and so on. It\u2019s also possible to build learning experiences like treasure hunts and biology or history learning games, or use this technology for situated art (visual art experiences bound to specific real world coordinates). Marker Tracking , When a marker is found by the camera, it's possible to show some content (same as Image Tracking). Markers are very stable but limited in shape, color and size. It is suggested for those experiences where are required a lot of different markers with different content. Examples of use: (Augmented books), Augmented flyers, advertising.","title":"AR types"},{"location":"#key-points","text":"Very Fast : It runs efficiently even on phones Web-based : It is a pure web solution, so no installation required. Full javascript based on three.js + A-Frame + jsartoolkit5 Open Source : It is completely open source and free of charge! Standards : It works on any phone with webgl and webrtc AR.js has reached version 3. This is the official repository: https://github.com/AR-js-org/AR.js . If you want to visit the old AR.js repository, here it is: https://github.com/jeromeetienne/AR.js .","title":"Key points"},{"location":"#import-the-library","text":"AR.js from version 3 has a new structure. AR.js is coming in two, different build. They are both maintained. They are exclusive. The file you want to import depends on what features you want, and also which render library you want to use (A-Frame or three.js). AR.js uses jsartoolkit5 for tracking, but can display augmented content with either three.js or A-Frame . You can import AR.js in one version of your choice, using the script tag on your HTML. AR.js with Image Tracking + Location Based AR Import AFRAME version: script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js Import three.js version: script src= https://raw.githack.com/AR-js-org/AR.js/master/three.js/build/ar-nft.js AR.js with Marker Tracking + Location Based AR: Import AFRAME version: script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js Import three.js version: script src= https://raw.githack.com/AR-js-org/AR.js/master/three.js/build/ar.js If you want to import a specific version, you can do that easily replacing master with the version tag, e.g.: script src= https://raw.githack.com/AR-js-org/AR.js/3.0.0/aframe/build/aframe-ar-nft.js","title":"Import the library"},{"location":"#requirements","text":"Some requirements and known restrictions are listed below: It works on every phone with webgl and webrtc . Marker based is very lightweight, while Image Tracking is more CPU consuming You cannot use Chrome on iOS, as Chrome on iOS did not support, at the moment, camera access On device with multi-cameras, Chrome may have problems on detecting the right one. Please use Firefox if you find that AR.js opens on the wrong camera. There is an open issue for this. To work with Location Based feature, your phone needs to have GPS sensors Please, read carefully any suggestions that AR.js pops-up -as alerts- for Location Based on iOS, as iOS requires user actions to activate geoposition Location Based feature is only available on A-Frame","title":"Requirements"},{"location":"#always-deploy-under-https","text":"Accessing to the phone camera or to camera GPS sensors, due to major browsers restrictions, can be done only under https websites. All the examples you will see, and all AR.js web apps in general, have to be run on a server. You can use local server or deploy the static web app on the web. So don't forget to always run your examples on secure connections servers or localhost. Github Pages is a great way to have free and live websites under https.","title":"Always deploy under https"},{"location":"#getting-started","text":"Here we present three, basic examples, one for each AR feature. For specific documentation, on the top menu you can find every section, or you can click on the following links: Image Tracking Documentation Location Based Documentation Marker Based Documentation","title":"Getting started"},{"location":"#image-tracking-example","text":"There is a Codepen for you to try. Below you can find also a live example. Please follow these simple steps: Create a new project with the code below (or open this live example and go directly to the last step) Run it on a server Open the website on your phone Scan this picture to see content through the camera. script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded -- div class= arjs-loader div Loading, please wait... /div /div a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- we use cors proxy to avoid cross-origin problems -- a-nft type= nft url= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/trex-image/trex smooth= true smoothCount= 10 smoothTolerance= .01 smoothThreshold= 5 a-entity gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf scale= 5 5 5 position= 50 150 0 /a-entity /a-nft a-entity camera /a-entity /a-scene /body","title":"Image Tracking Example"},{"location":"#location-based-example","text":"Try it live with this Codepen . It retrieves your position and places a text near you. Please follow these simple steps: Create a new project with the following snippet, and change add-your-latitude and add-your-longitude with your latitude and longitude, without the . Run it on a server Activate GPS on your phone and navigate to the example URL Look around. You should see the text looking at you, appearing in the requested position, even if you look around and move the phone. !DOCTYPE html html head meta charset= utf-8 / meta http-equiv= X-UA-Compatible content= IE=edge / title GeoAR.js demo /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script /head body style= margin: 0; overflow: hidden; a-scene vr-mode-ui= enabled: false embedded arjs= sourceType: webcam; debugUIEnabled: false; a-text value= This content will always face you. look-at= [gps-camera] scale= 120 120 120 gps-entity-place= latitude: add-your-latitude ; longitude: add-your-longitude ; /a-text a-camera gps-camera rotation-reader /a-camera /a-scene /body /html If you want to enhance and customize your Location Based experience, take a look at the Location Based docs.","title":"Location Based Example"},{"location":"#marker-based-example","text":"Please follow these simple steps: Create a new project with the code below (or open this live example and go directly to the last step) Run it on a server Open the website on your phone Scan this picture to see content through the camera. !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script body style= margin : 0px; overflow: hidden; a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html","title":"Marker Based Example"},{"location":"#advanced-stuff","text":"AR.js offers two ways, with A-Frame, to interact with the web page: to interact directly with AR content and Overlayed DOM interaction. Also, there are several Custom Events triggered during the life cycle of every AR.js web app. You can learn more about these aspects on the UI and Events section .","title":"Advanced stuff"},{"location":"#arjs-architecture","text":"AR.js uses jsartoolkit5 for tracking, but can display augmented content with either three.js or A-Frame . three.js folder contains source code for AR.js core, Marker based and Image Tracking examples for AR.js three.js based build for three.js AR.js based vendor stuff (jsartoolkit5) workers (used for Image Tracking). When you find files that ends with -nft suffix, they're boundled only with the Image Tracking version. A-Frame version of AR.js uses three.js parts as its core. A-Frame code, on AR.js, is simply a wrapper to write AR with Custom Components in HTML. aframe folder contains source code for AR.js A-Frame (aka wrappers for Marker Based, Image Tracking components) source code for Location Based build for A-Frame AR.js based examples for A-Frame AR.js.","title":"AR.js architecture"},{"location":"#tutorials","text":"There are various tutorials available for developing with AR.js. These include:","title":"Tutorials"},{"location":"#location-based","text":"Build your Location-Based Augmented Reality Web App Develop a Simple Peakfinder App ( Provided with these docs )","title":"Location Based"},{"location":"#troubleshooting-feature-requests-community","text":"You can find a lot of help on the old AR.js repositories issues . Please search on open/closed issues, you may find a interesting stuff.","title":"Troubleshooting, feature requests, community"},{"location":"#contributing","text":"From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.","title":"Contributing"},{"location":"#issues","text":"If you are having configuration or setup problems, please post a question to StackOverflow . You can also address question to us in our Gitter chatroom If you have discovered a bug or have a feature suggestion, feel free to create an issue on Github.","title":"Issues"},{"location":"#submitting-changes","text":"After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly. Some things that will increase the chance that your pull request is accepted: Follow the existing coding style Write a good commit message","title":"Submitting Changes"},{"location":"about/","text":"Aknowledgments This project has been created by @jeromeetienne and it is now maintained by @nicolocarpignoli and the AR.js Org Community. Notes about AR.js 3 release: After months of work, we have changed AR.js for good. The aim was to make it a true, free alternative to paid Web AR solutions. We don't know if we're already there, but now the path is clear, at least. We have worked hard, spent many days and nights\u200a-\u200aobviously, we are coders, what did you expect?\u200a-\u200aand we are now so thrilled to share this achievement with the community. We know that it can be better, we know its limitations, but we would love to share this journey's result. AR.js is now under a Github organisation, that means, more collaborative than ever. It has a new structure, and a lot of new code. And most of all, we've added Image Tracking, what we felt was the missing piece for a true alternative to Web AR. A huge, huge thanks to the wonderful guys who made this possible: Walter Perdan Thorsten Bux Daniel Fernandes misdake hatsumatsu and many more. It was great to built this with all of you.","title":"About"},{"location":"about/#aknowledgments","text":"This project has been created by @jeromeetienne and it is now maintained by @nicolocarpignoli and the AR.js Org Community. Notes about AR.js 3 release: After months of work, we have changed AR.js for good. The aim was to make it a true, free alternative to paid Web AR solutions. We don't know if we're already there, but now the path is clear, at least. We have worked hard, spent many days and nights\u200a-\u200aobviously, we are coders, what did you expect?\u200a-\u200aand we are now so thrilled to share this achievement with the community. We know that it can be better, we know its limitations, but we would love to share this journey's result. AR.js is now under a Github organisation, that means, more collaborative than ever. It has a new structure, and a lot of new code. And most of all, we've added Image Tracking, what we felt was the missing piece for a true alternative to Web AR. A huge, huge thanks to the wonderful guys who made this possible: Walter Perdan Thorsten Bux Daniel Fernandes misdake hatsumatsu and many more. It was great to built this with all of you.","title":"Aknowledgments"},{"location":"image-tracking/","text":"Image Tracking Image Tracking makes possible to scan a picture, a drawing, any image, and show content over it. All the following examples are with A-Frame, for semplicity. You can use three.js if you want. See on the official repository the nft three.js example . All A-Frame examples for Image Tracking can be found here . Getting started with Image Tracking Natural Feature Tracking or NFT is a technology that enables the use of images instead of markers like QR Codes or the Hiro marker. The software tracks interesting points in the image and using them, it estimates the position of the camera. These interesting points (aka \"Image Descriptors\") are created using the NFT Marker Creator , a tool available for creating NFT markers. It comes in two versions: the Web version (recommended), and the node.js version . There is also a fork of this project on the AR.js Github organisation, but as for now, Daniel Fernandes version works perfectly. Thanks to Daniel Fernandes for contribution on this docs section. Choose good images If you want to understand the creation of markers in more depth, check out the NFT Marker Creator wiki . It explains also why certain images work way better than others. An important factor is the DPI of the image: a good dpi (300 or more) will give a very good stabilization, while low DPI (like 72) will require the user to stay very still and close to the image, otherwise tracking will lag. Create Image Descriptors Once you have chosen your image, you can either use the NFT Marker Creator in its Web version or the node version. If you're using the node version, this is the basic command to run: node app.js -i path-to-the-img/image-name.jpg/png After that, you will find the Image Descriptors files on the output folder. In the web version, the generator will automatically download the files from your browser. In either cases, you will end up with three files as Image Descriptors, with .fset , .fset3 , .iset . Each of them will have the same prefix before the file extension. That one will be the Image Descriptor name that you will use on the AR.js web app. For example: with files trex.fset , trex.fset3 and trex.iset , your Image Descriptors name will be trex . Render the content Now it's time to create the actual AR web app. !-- import aframe and then ar.js with image tracking / location based features -- script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script !-- style for the loader -- style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -- div class= arjs-loader div Loading, please wait... /div /div !-- a-frame scene -- a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- a-nft is the anchor that defines an Image Tracking entity -- !-- on 'url' use the path to the Image Descriptors created before. -- !-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -- a-nft type= nft url= path-to-your-image-descriptors smooth= true smoothCount= 10 smoothTolerance= .01 smoothThreshold= 5 !-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -- a-entity gltf-model= path-to-your-model scale= 5 5 5 position= 50 150 0 /a-entity /a-nft !-- static camera that moves according to the device movemenents -- a-entity camera /a-entity /a-scene /body See on the comments above, inline on the code, for explanations. You can refer to A-Frame docs to know everything about content and customization. You can add geometries, 3D models, videos, images. And you can customize their position, scale, rotation and so on. The only custom component here is the a-nft , the Image Tracking HTML anchor. a-nft\\ Here are the attributes for this entity Attribute Description Component Mapping type type of marker - ['nft' only valid value] artoolkitmarker.type url url of the Image Descriptors, without extension artoolkitmarker.descriptorsUrl emitevents emits 'markerFound' and 'markerLost' events - ['true', 'false'] - smooth turn on/off camera smoothing - ['true', 'false'] - default: false - smoothCount number of matrices to smooth tracking over, more = smoother but slower follow - default: 5 - smoothTolerance distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still - default: 0.01 - smoothThreshold threshold for smoothing, will keep still unless enough matrices are over tolerance - default: 2 - size size of the marker in meter artoolkitmarker.size \u26a1\ufe0f It is suggested to use smooth , smoothCount and smoothTolerance because of weak stabilization of content in Image Tracking. Thanks to smoothing, content is way more stable, from 3D models to 2D videos.","title":"Image Tracking"},{"location":"image-tracking/#image-tracking","text":"Image Tracking makes possible to scan a picture, a drawing, any image, and show content over it. All the following examples are with A-Frame, for semplicity. You can use three.js if you want. See on the official repository the nft three.js example . All A-Frame examples for Image Tracking can be found here .","title":"Image Tracking"},{"location":"image-tracking/#getting-started-with-image-tracking","text":"Natural Feature Tracking or NFT is a technology that enables the use of images instead of markers like QR Codes or the Hiro marker. The software tracks interesting points in the image and using them, it estimates the position of the camera. These interesting points (aka \"Image Descriptors\") are created using the NFT Marker Creator , a tool available for creating NFT markers. It comes in two versions: the Web version (recommended), and the node.js version . There is also a fork of this project on the AR.js Github organisation, but as for now, Daniel Fernandes version works perfectly. Thanks to Daniel Fernandes for contribution on this docs section.","title":"Getting started with Image Tracking"},{"location":"image-tracking/#choose-good-images","text":"If you want to understand the creation of markers in more depth, check out the NFT Marker Creator wiki . It explains also why certain images work way better than others. An important factor is the DPI of the image: a good dpi (300 or more) will give a very good stabilization, while low DPI (like 72) will require the user to stay very still and close to the image, otherwise tracking will lag.","title":"Choose good images"},{"location":"image-tracking/#create-image-descriptors","text":"Once you have chosen your image, you can either use the NFT Marker Creator in its Web version or the node version. If you're using the node version, this is the basic command to run: node app.js -i path-to-the-img/image-name.jpg/png After that, you will find the Image Descriptors files on the output folder. In the web version, the generator will automatically download the files from your browser. In either cases, you will end up with three files as Image Descriptors, with .fset , .fset3 , .iset . Each of them will have the same prefix before the file extension. That one will be the Image Descriptor name that you will use on the AR.js web app. For example: with files trex.fset , trex.fset3 and trex.iset , your Image Descriptors name will be trex .","title":"Create Image Descriptors"},{"location":"image-tracking/#render-the-content","text":"Now it's time to create the actual AR web app. !-- import aframe and then ar.js with image tracking / location based features -- script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script !-- style for the loader -- style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded. Loading may take a while according to the device computational power -- div class= arjs-loader div Loading, please wait... /div /div !-- a-frame scene -- a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- a-nft is the anchor that defines an Image Tracking entity -- !-- on 'url' use the path to the Image Descriptors created before. -- !-- the path should end with the name without the extension e.g. if file is 'pinball.fset' the path should end with 'pinball' -- a-nft type= nft url= path-to-your-image-descriptors smooth= true smoothCount= 10 smoothTolerance= .01 smoothThreshold= 5 !-- as a child of the a-nft entity, you can define the content to show. here's a GLTF model entity -- a-entity gltf-model= path-to-your-model scale= 5 5 5 position= 50 150 0 /a-entity /a-nft !-- static camera that moves according to the device movemenents -- a-entity camera /a-entity /a-scene /body See on the comments above, inline on the code, for explanations. You can refer to A-Frame docs to know everything about content and customization. You can add geometries, 3D models, videos, images. And you can customize their position, scale, rotation and so on. The only custom component here is the a-nft , the Image Tracking HTML anchor.","title":"Render the content"},{"location":"image-tracking/#lta-nftgt","text":"Here are the attributes for this entity Attribute Description Component Mapping type type of marker - ['nft' only valid value] artoolkitmarker.type url url of the Image Descriptors, without extension artoolkitmarker.descriptorsUrl emitevents emits 'markerFound' and 'markerLost' events - ['true', 'false'] - smooth turn on/off camera smoothing - ['true', 'false'] - default: false - smoothCount number of matrices to smooth tracking over, more = smoother but slower follow - default: 5 - smoothTolerance distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still - default: 0.01 - smoothThreshold threshold for smoothing, will keep still unless enough matrices are over tolerance - default: 2 - size size of the marker in meter artoolkitmarker.size \u26a1\ufe0f It is suggested to use smooth , smoothCount and smoothTolerance because of weak stabilization of content in Image Tracking. Thanks to smoothing, content is way more stable, from 3D models to 2D videos.","title":"&lt;a-nft\\&gt;"},{"location":"location-based-tutorial/","text":"AR.js Location-Based Tutorial - Develop a Simple Peakfinder App Introduction This tutorial aims to take you from a basic location-based AR.js example all the way to a working, simple peak-finder app. We will start with an HTML-only example and gradually add JavaScript to make our app more sophisticated. It is expected that you have some basic A-Frame experience. Basic example We will start with a basic example, using pure HTML, to display a box close to your location. This example is similar to the location-based example on the index page . html head title AR.js Basic Projected Camera Example /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script !-- Look-at component. We don't need this now, but we will later. -- script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script /head body a-scene vr-mode-ui= enabled: false arjs='sourceType: webcam; videoTexture: true; debugUIEnabled: false;' renderer='antialias: true; alpha: true' a-camera gps-projected-camera rotation-reader /a-camera a-box gps-projected-entity-place='latitude: your-lat; longitude: your-lon' material='color: red' scale='10 10 10' /a-box /a-scene /body /html Upload this to a server with HTTPS, or run locally on localhost . Make sure you replace your-lat and your-lon with values close to your actual position (to see the box clearly, I would recommend an offset of around 0.0004 degrees in any direction for both the latitude and longitude). How does this work? This is simplar to the example on the index page . The arjs component of our a-scene initialises AR.js. Note the properties we are setting: we set the sourceType to webcam for obvious reasons but also set videoTexture to true. This is vital in an outdoor location-based AR app as it allows distant augmented content - such at the peaks we are going to eventually visualise - to be seen. (It does this by using a three.js texture for the camera feed which can be easily combined with our augmented content). Note the gps-projected-camera component on our a-camera . This is the AR.js component which automatically converts latitudes and longitudes into 3D world coordinates, allowing us to use latitude and longitude, rather than world coordinates, when adding places. Note that we are using gps-projected-camera , not gps-camera . The gps-projected-camera component makes it easy for us to work with arbitrary geographical data provided by a server, as internally it uses the Spherical Mercator projection to represent the augmented content's world coordinates. Spherical Mercator units are commonly used to represent mapping data and are almost (but not quite) equivalent to metres. Away from the polar regions, though, it's good enough to use for AR. We then create an a-box primitive. This is the augmented content that we want to display. Ordinarily, in A-Frame, you would give this a position in world coordinates. However, AR.js, and specifically the gps-projected-entity-place component, allows us to position it using latitude and longitude. We can position any A-Frame entity at a given latitude and longitude using gps-projected-entity-place . Things to try Change the a-box to some other kind of A-Frame primitive, such as an a-sphere or a-cylinder . Does it still work? Try adding multiple objects with different colours at different locations. Try adding a text primitive at a nearby latitude and longitude. Use the location-based example on the index.page to help you. You will need to use the A-Frame look-at component to ensure the text always faces the camera. Try giving your objects an elevation. This can be done by setting the y coordinate of the position property of each object to a given height (in metres) and setting the x and z coordinates to 0. Having done that, try giving the camera an elevation by similarly setting its position property, and look at the effect this has on where the objects appear. Introducing JavaScript with AR.js Much of the power of A-Frame, and AR.js, comes from adding scripting to your basic applications. It is assumed that you already know the basics of how to create components in A-Frame, but let's start with a refresher. Create this file, peakfinder.js : AFRAME.registerComponent('peakfinder', { init: function() { alert('Peakfinder component initialising!'); } }); and link to it from your HTML by adding, in the head section after the A-Frame and AR.js scripts: script type='text/javascript' src='peakfinder.js' /script Also add a new entity to your scene and add this component to it: a-entity peakfinder /a-entity Load your page again. You should see the alert box come up. Remember from basic A-Frame that to create a component, you need to register it as in the example above. Remember also that the init() function runs when the component is first initialised, and you add components to entities by making them attributes of the a-entity tag. Connecting to a web API Now we've got a basic component set up, we are going to make it do something: connect to a web API to retrieve the locations of nearby peaks - with our eventual aim of making a peakfinder app. There are various APIs which can be used, but we will use one which is based on OpenStreetMap data. This API is present on the Hikar web server and delivers the peak data as GeoJSON, but only covers Europe, Turkey and Washington State, USA, due to server capacity constraints. However, please feel free to research alternative APIs which cover other parts of the world and adjust the code accordingly. Also please feel free to contact me(@nickw1 on GitHub) if you want to add your area of the world (please do not ask, however, for large, heavily-populated countries in their entirety; to give you an idea of the size I could accommodate, the Washington State coverage was the result of a request). Also, note that the API is open source so again contact me for guidance on how you can set it up on your own server to cover your area of the world. Modify your code as follows: AFRAME.registerComponent('peakfinder', { init: function() { this.loaded = false; window.addEventListener('gps-camera-update-position', e = { if(this.loaded === false) { this._loadPeaks(e.detail.position.longitude, e.detail.position.latitude); this.loaded = true; } }); } }); What is this doing? We are listening for the gps-camera-update-position event. This is emitted by gps-projected-camera whenever our GPS location changes. The detail of the event contains a position object with longitude and latitude properties which contain our current position. So when we have obtained our position, we call an (as yet unwritten) _loadPeaks() method which will actually download the locations of the peaks from a web API. Note also the loaded boolean. This is used to prevent the peaks being loaded every time the position changes , which will clearly result in unnecessary network communication. For now, we just download the peaks once, when the application is initialised. So next, we will write the _loadPeaks() method. Add this as a new method to your component: _loadPeaks: function(longitude, latitude) { const scale = 2000; fetch(`https://www.hikar.org/fm/ws/bsvr.php?bbox=${longitude-0.1},${latitude-0.1},${longitude+0.1},${latitude+0.1} outProj=4326 format=json poi=natural` ) .then ( response = response.json() ) .then ( json = { json.features.filter ( f = f.properties.natural == 'peak' ) .forEach ( peak = { const entity = document.createElement('a-text'); entity.setAttribute('look-at', '[gps-projected-camera]'); entity.setAttribute('value', peak.properties.name); entity.setAttribute('scale', { x: scale, y: scale, z: scale }); entity.setAttribute('gps-projected-entity-place', { latitude: peak.geometry.coordinates[1], longitude: peak.geometry.coordinates[0] }); this.el.appendChild(entity); }); }); } How is this working? We first send a standard AJAX request to our API, using fetch , and using a bounding box of width and height 0.2 degrees centered on our current location. We obtain JSON from the response using the standard fetch promise-based approach. Once we have our JSON (specifically, GeoJSON) as an array of features, we filter them to select only peaks. For each peak, we: Create an a-text primitive and set it to look-at the gps-projected-camera component (the camera). This will ensure that the text will always face the user. Ensure that you have linked the source code of the A-Frame look-at component! Set the text's value to the peak's name. In other words, the name of each peak will be displayed. Scale the text appropriately. Add the gps-projected-entity-place component to our a-text to position the text using latitude and longitude. The coordinates are extracted from the GeoJSON. Using standard DOM, append the a-text to the parent element of our peakfinder component. Try this out, making sure you do so in an area where there are at least small hills, which are likely to be on OpenStreetMap. You can check OpenStreetMap and look for peak symbols near you on the map. If necessary, expand the bounding box. You will find that the text should appear at the correct place for the peak, but with one big problem - elevation is not included. So far, we are only using latitude and longitude to place the peak. Clearly, a fully-working peakfinder app will include elevation too! The next tutorial section addresses this. Nonetheless, even without elevation, hopefully this tutorial will show you how easy it is to generate dynamic location-based AR content using AR.js. Things to try Rather than hard-coding the scale at 2000, add it as a property using the component's schema, and experiment with different values by setting the property in your HTML. Adding elevation, and tiling As discussed, we would like our peakfinder app to show the peaks at the correct elevation . Another issue with any outdoor AR app is that we want, ideally, to download new data as we move to new areas so that, for example, peaks within 10km of us are downloaded when we start the app, and then as we reach the edge of the already-downloaded area, new peaks are downloaded. At the moment, we simply download the peaks when we start the app, and do not update them. These issues provide much of the complexity of a location-based AR app. Luckily, though, a pre-built solution to both problems exists. One of the great things about A-Frame is the fact that there are many pre-built components which can be added to our app , and two pre-built components exists for this precise problem: terrarium-dem and osm3d , both part of the aframe-osm-3d package. How the aframe-osm-3d components work terrarium-dem downloads Digital Elevation Model (DEM) data for a given location. This data was based on NASA SRTM data, and was converted into the Terrarium PNG format by the former mapping company Mapzen (see original article ). As discussed in this article, elevation is encoded in the red, green, and blue channels of a PNG image, and is now available, without usage restrictions, on AWS . The raw elevation data is emitted within an event (see below) and optionally, the terrain is rendered as a 3D mesh. osm3d is used to download OSM data from a GeoJSON data structure in 3D, in other words, the coordinates of the OSM features contain not just latitude and longitude, but elevation in metres. The elevation is obtained from the DEM emitted from terrarium-dem , above; osm3d automatically listens for the terrarium-dem-loaded event emitted from terrarium-dem and uses the DEM to calculate the elevation. You can find out how these components work in more detail by visiting their GitHub repository . Tiling The aframe-osm-3d components discussed above both work using a tiling system. Regions of the world are split up into tiles , using the \"XYZ\" or \"Google\" tiling system, which you can read about in more detail here . The general idea is that each tile is defined by an X, a Y, and a Z coordinate, in which: X represents the tile's coordinate on a west-east axis; Y represents the tile's coordinate on a north-south axis; Z represents the tile's zoom level, discussed in more detail in the link above. terrarium-dem reads in a given longitude and latitude, and downloads the current XYZ elevation tile at a given zoom level, and the eight surrounding tiles (nine in total). These tiles are internally cached by terrarium-dem so that if a nearby location is requested, and the user hasn't moved to a different tile, the cached data will be used, avoiding unnecessary downloads. Also, when tiles are downloaded, they are emitted within the terrarium-dem-loaded event, which the osm3d component responds to by downloading the corresponding tiles from an OSM web API, calculating the OSM data's elevation using the DEM, and again caching and emitting the result. The code With that in mind, here is our code which makes use of terrarium-dem and osm3d to download elevation and OSM data to create peaks with elevations. We will start with the HTML: !DOCTYPE html html head meta charset= utf-8 / title AR.js Peak Finder /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script script src= js/bundle.js /script /head body a-scene vr-mode-ui= enabled: false arjs='sourceType: webcam; videoTexture: true; debugUIEnabled: false;' a-camera gps-projected-camera rotation-reader /a-camera a-entity terrarium-dem='zoom: 12; url: proxy.php?x={x} y={y} z={z}' osm3d='url: https://hikar.org/fm/ws/tsvr.php?x={x} y={y} z={z} amp;poi=natural amp;outProj=4326' peakfinder /a-entity /a-scene /body /html What's new here? Note that we are linking in our JavaScript (which we will come to shortly) as a bundle . This is a great way of developing code which relies on third-party modules without having to link multiple JS files, and we will expand upon this below. Note how our custom entity now has some additional components: the terrarium-dem and osm3d components which we discussed above. Note how each component has a url property defining the source URL to download the data. Note also how we set the zoom to 12 for terrarium-dem . This will use the zoom level of 12 for our tiles, which means that peaks several kilometres away in every direction will be downloaded on startup. You'll notice that the url property for terrarium-dem is not an AWS URL but rather a local URL, proxy.php . Why is this? Due to the same-origin-policy we cannot contact AWS directly via AJAX, so we need to create a proxy script to do it for us. The example provided is in PHP and will look something like the code below. (If your server does not have PHP installed, feel free to replace this code with another language). ?php $x = $_GET[ x ]; $y = $_GET[ y ]; $z = $_GET[ z ]; if(ctype_digit($x) ctype_digit($y) ctype_digit($z)) { header( Content-Type: image/png ); echo file_get_contents( https://s3.amazonaws.com/elevation-tiles-prod/terrarium/$z/$x/$y.png ); } else { header( HTTP/1.1 400 Bad Request ); header( Content-Type: application/json ); echo json_encode([ error = invalid x, y and/or z params ]); } ? For the OSM URL (on hikar.org ), we do not need a proxy as this script has CORS enabled by default. We can now move on to our JavaScript. This will look something like this: require('aframe-osm-3d'); AFRAME.registerComponent('peakfinder', { schema: { scale: { type: 'number', default: 15 } }, init: function() { this.textScale = this.data.scale * 100; this.camera = document.querySelector('a-camera'); window.addEventListener('gps-camera-update-position', e = { this.el.setAttribute('terrarium-dem', { lat: e.detail.position.latitude, lon: e.detail.position.longitude }) }); this.el.addEventListener('elevation-available', e = { const position = this.camera.getAttribute('position'); position.y = e.detail.elevation + 1.6; this.camera.setAttribute('position', position); }); this.el.addEventListener('osm-data-loaded', e = { e.detail.pois .filter ( f = f.properties.natural == 'peak' ) .forEach ( peak = { const entity = document.createElement('a-entity'); entity.setAttribute('look-at', '[gps-projected-camera]'); const text = document.createElement('a-text'); text.setAttribute('value', peak.properties.name); text.setAttribute('scale', { x: this.textScale, y: this.textScale, z: this.textScale }); text.setAttribute('align', 'center'); text.setAttribute('position', { x: 0, y: this.data.scale * 20, z: 0 }); entity.setAttribute('gps-projected-entity-place', { latitude: peak.geometry.coordinates[1], longitude: peak.geometry.coordinates[0] }); entity.setAttribute('position', { x: 0, y: peak.geometry.coordinates[2], z: 0 }); entity.appendChild(text); const cone = document.createElement('a-cone'); cone.setAttribute('radiusTop', 0.1); cone.setAttribute('scale', { x: this.data.scale * 10, y: this.data.scale * 10, z: this.data.scale * 10 }); cone.setAttribute('height', 3); cone.setAttribute('material', { color: 'magenta' } ); entity.appendChild(cone); this.el.appendChild(entity); }); }); } }); How does this work? We start by require ing the aframe-osm-3d package. This is a package available on NPM , and as we have seen, contains the terrarium-dem and osm3d components. aframe-osm-3d is a JavaScript module , specifically a CommonJS module, and this technique of require ing modules is standard in Node.js. However with the help of tools such as Browserify , we can use modules in the browser, too. More on this below. We then begin our actual component. Note how its schema includes a scale , allowing us to control the scale of the rendered peaks. Feel free to experiment with changing this. We then move on to our init() . This works a bit differently this time: We once again handle the gps-camera-update-position event, but this time we pass the latitude and longitude onto our terrarium-dem component. As we saw above, this will download the local tiles for this position. We next handle the elevation-available event, which is emitted from our terrarium-dem component as soon as we have elevation available for our current position. The elevation is contained in the emitted event; we handle it by setting the camera's y coordinate to that elevation plus 1.6 (to account for the fact that a user will be holding the device above the ground0. We then handle the osm-data-loaded event. This will be emitted from osm3d whenever new data has been downloaded from the OSM API and elevation added. This event contains various properties, but we will use the pois property which is an array of GeoJSON features, one for each POI. Note how we filter the POIs to select peaks only, once again. We then create one entity per peak, as before. We ensure the entity is looking at the camera, and then create two child entities, an a-cone primitive to show a peak-like graphic, and an a-text primitive to show the peak's name. We align the text to the centre so that it's correctly placed relative to the cone, and set coordinates and scale appropriately. We also add a gps-projected-entity-place component to each entity, as before, so that we can position them using their latitude and longitude. Note how, once we have set the latitude and longitude with gps-projected-entity-place , we set the elevation of each peak entity. We do this using the third member of the coordinates array contained within the GeoJSON, which will contain the peak's elevation in metres. And that is all that is needed to create a very simple peak finder in AR.js! We now need to build the peakfinder, as indicated above. First, we need to install the aframe-osm-3d package with NPM (included with Node.js ). npm install aframe-osm-3d Next, we need to create a bundle with Browserify . Browserify is an example of a \"bundling\" tool. It takes multiple JavaScript files, including third-party modules, and produces a single bundle , which can be linked to from an HTML page. To use, it's simply: browserify peakfinder.js bundle.js (assuming that you saved your peakfinder component as peakfinder.js ). And that is it! Try it out and see if you can see peaks in their correct location, and elevation. Further things to try It would be nice to show users status messages as the peaks are downloading. You can do this by handling a couple of other events: terrarium-start-update is emitted when we start to download new DEM data. terrarium-dem-loaded , as we have already seen, is emitted when DEM data has finished downloading. Adapt your example to include a div which displays a status message. (You can make a div a child of your a-scene ). Display \"Downloading elevation data...\" while it's downloading the DEM data, and then \"Downloading OSM data\" when the DEM download has finished. Finally, when the OSM data has loaded, set the div 's contents to a blank string.","title":"AR.js Location-Based Tutorial - Develop a Simple Peakfinder App"},{"location":"location-based-tutorial/#arjs-location-based-tutorial-develop-a-simple-peakfinder-app","text":"","title":"AR.js Location-Based Tutorial - Develop a Simple Peakfinder App"},{"location":"location-based-tutorial/#introduction","text":"This tutorial aims to take you from a basic location-based AR.js example all the way to a working, simple peak-finder app. We will start with an HTML-only example and gradually add JavaScript to make our app more sophisticated. It is expected that you have some basic A-Frame experience.","title":"Introduction"},{"location":"location-based-tutorial/#basic-example","text":"We will start with a basic example, using pure HTML, to display a box close to your location. This example is similar to the location-based example on the index page . html head title AR.js Basic Projected Camera Example /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script !-- Look-at component. We don't need this now, but we will later. -- script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script /head body a-scene vr-mode-ui= enabled: false arjs='sourceType: webcam; videoTexture: true; debugUIEnabled: false;' renderer='antialias: true; alpha: true' a-camera gps-projected-camera rotation-reader /a-camera a-box gps-projected-entity-place='latitude: your-lat; longitude: your-lon' material='color: red' scale='10 10 10' /a-box /a-scene /body /html Upload this to a server with HTTPS, or run locally on localhost . Make sure you replace your-lat and your-lon with values close to your actual position (to see the box clearly, I would recommend an offset of around 0.0004 degrees in any direction for both the latitude and longitude).","title":"Basic example"},{"location":"location-based-tutorial/#how-does-this-work","text":"This is simplar to the example on the index page . The arjs component of our a-scene initialises AR.js. Note the properties we are setting: we set the sourceType to webcam for obvious reasons but also set videoTexture to true. This is vital in an outdoor location-based AR app as it allows distant augmented content - such at the peaks we are going to eventually visualise - to be seen. (It does this by using a three.js texture for the camera feed which can be easily combined with our augmented content). Note the gps-projected-camera component on our a-camera . This is the AR.js component which automatically converts latitudes and longitudes into 3D world coordinates, allowing us to use latitude and longitude, rather than world coordinates, when adding places. Note that we are using gps-projected-camera , not gps-camera . The gps-projected-camera component makes it easy for us to work with arbitrary geographical data provided by a server, as internally it uses the Spherical Mercator projection to represent the augmented content's world coordinates. Spherical Mercator units are commonly used to represent mapping data and are almost (but not quite) equivalent to metres. Away from the polar regions, though, it's good enough to use for AR. We then create an a-box primitive. This is the augmented content that we want to display. Ordinarily, in A-Frame, you would give this a position in world coordinates. However, AR.js, and specifically the gps-projected-entity-place component, allows us to position it using latitude and longitude. We can position any A-Frame entity at a given latitude and longitude using gps-projected-entity-place .","title":"How does this work?"},{"location":"location-based-tutorial/#things-to-try","text":"Change the a-box to some other kind of A-Frame primitive, such as an a-sphere or a-cylinder . Does it still work? Try adding multiple objects with different colours at different locations. Try adding a text primitive at a nearby latitude and longitude. Use the location-based example on the index.page to help you. You will need to use the A-Frame look-at component to ensure the text always faces the camera. Try giving your objects an elevation. This can be done by setting the y coordinate of the position property of each object to a given height (in metres) and setting the x and z coordinates to 0. Having done that, try giving the camera an elevation by similarly setting its position property, and look at the effect this has on where the objects appear.","title":"Things to try"},{"location":"location-based-tutorial/#introducing-javascript-with-arjs","text":"Much of the power of A-Frame, and AR.js, comes from adding scripting to your basic applications. It is assumed that you already know the basics of how to create components in A-Frame, but let's start with a refresher. Create this file, peakfinder.js : AFRAME.registerComponent('peakfinder', { init: function() { alert('Peakfinder component initialising!'); } }); and link to it from your HTML by adding, in the head section after the A-Frame and AR.js scripts: script type='text/javascript' src='peakfinder.js' /script Also add a new entity to your scene and add this component to it: a-entity peakfinder /a-entity Load your page again. You should see the alert box come up. Remember from basic A-Frame that to create a component, you need to register it as in the example above. Remember also that the init() function runs when the component is first initialised, and you add components to entities by making them attributes of the a-entity tag.","title":"Introducing JavaScript with AR.js"},{"location":"location-based-tutorial/#connecting-to-a-web-api","text":"Now we've got a basic component set up, we are going to make it do something: connect to a web API to retrieve the locations of nearby peaks - with our eventual aim of making a peakfinder app. There are various APIs which can be used, but we will use one which is based on OpenStreetMap data. This API is present on the Hikar web server and delivers the peak data as GeoJSON, but only covers Europe, Turkey and Washington State, USA, due to server capacity constraints. However, please feel free to research alternative APIs which cover other parts of the world and adjust the code accordingly. Also please feel free to contact me(@nickw1 on GitHub) if you want to add your area of the world (please do not ask, however, for large, heavily-populated countries in their entirety; to give you an idea of the size I could accommodate, the Washington State coverage was the result of a request). Also, note that the API is open source so again contact me for guidance on how you can set it up on your own server to cover your area of the world. Modify your code as follows: AFRAME.registerComponent('peakfinder', { init: function() { this.loaded = false; window.addEventListener('gps-camera-update-position', e = { if(this.loaded === false) { this._loadPeaks(e.detail.position.longitude, e.detail.position.latitude); this.loaded = true; } }); } }); What is this doing? We are listening for the gps-camera-update-position event. This is emitted by gps-projected-camera whenever our GPS location changes. The detail of the event contains a position object with longitude and latitude properties which contain our current position. So when we have obtained our position, we call an (as yet unwritten) _loadPeaks() method which will actually download the locations of the peaks from a web API. Note also the loaded boolean. This is used to prevent the peaks being loaded every time the position changes , which will clearly result in unnecessary network communication. For now, we just download the peaks once, when the application is initialised. So next, we will write the _loadPeaks() method. Add this as a new method to your component: _loadPeaks: function(longitude, latitude) { const scale = 2000; fetch(`https://www.hikar.org/fm/ws/bsvr.php?bbox=${longitude-0.1},${latitude-0.1},${longitude+0.1},${latitude+0.1} outProj=4326 format=json poi=natural` ) .then ( response = response.json() ) .then ( json = { json.features.filter ( f = f.properties.natural == 'peak' ) .forEach ( peak = { const entity = document.createElement('a-text'); entity.setAttribute('look-at', '[gps-projected-camera]'); entity.setAttribute('value', peak.properties.name); entity.setAttribute('scale', { x: scale, y: scale, z: scale }); entity.setAttribute('gps-projected-entity-place', { latitude: peak.geometry.coordinates[1], longitude: peak.geometry.coordinates[0] }); this.el.appendChild(entity); }); }); } How is this working? We first send a standard AJAX request to our API, using fetch , and using a bounding box of width and height 0.2 degrees centered on our current location. We obtain JSON from the response using the standard fetch promise-based approach. Once we have our JSON (specifically, GeoJSON) as an array of features, we filter them to select only peaks. For each peak, we: Create an a-text primitive and set it to look-at the gps-projected-camera component (the camera). This will ensure that the text will always face the user. Ensure that you have linked the source code of the A-Frame look-at component! Set the text's value to the peak's name. In other words, the name of each peak will be displayed. Scale the text appropriately. Add the gps-projected-entity-place component to our a-text to position the text using latitude and longitude. The coordinates are extracted from the GeoJSON. Using standard DOM, append the a-text to the parent element of our peakfinder component. Try this out, making sure you do so in an area where there are at least small hills, which are likely to be on OpenStreetMap. You can check OpenStreetMap and look for peak symbols near you on the map. If necessary, expand the bounding box. You will find that the text should appear at the correct place for the peak, but with one big problem - elevation is not included. So far, we are only using latitude and longitude to place the peak. Clearly, a fully-working peakfinder app will include elevation too! The next tutorial section addresses this. Nonetheless, even without elevation, hopefully this tutorial will show you how easy it is to generate dynamic location-based AR content using AR.js.","title":"Connecting to a web API"},{"location":"location-based-tutorial/#things-to-try_1","text":"Rather than hard-coding the scale at 2000, add it as a property using the component's schema, and experiment with different values by setting the property in your HTML.","title":"Things to try"},{"location":"location-based-tutorial/#adding-elevation-and-tiling","text":"As discussed, we would like our peakfinder app to show the peaks at the correct elevation . Another issue with any outdoor AR app is that we want, ideally, to download new data as we move to new areas so that, for example, peaks within 10km of us are downloaded when we start the app, and then as we reach the edge of the already-downloaded area, new peaks are downloaded. At the moment, we simply download the peaks when we start the app, and do not update them. These issues provide much of the complexity of a location-based AR app. Luckily, though, a pre-built solution to both problems exists. One of the great things about A-Frame is the fact that there are many pre-built components which can be added to our app , and two pre-built components exists for this precise problem: terrarium-dem and osm3d , both part of the aframe-osm-3d package.","title":"Adding elevation, and tiling"},{"location":"location-based-tutorial/#how-the-aframe-osm-3d-components-work","text":"terrarium-dem downloads Digital Elevation Model (DEM) data for a given location. This data was based on NASA SRTM data, and was converted into the Terrarium PNG format by the former mapping company Mapzen (see original article ). As discussed in this article, elevation is encoded in the red, green, and blue channels of a PNG image, and is now available, without usage restrictions, on AWS . The raw elevation data is emitted within an event (see below) and optionally, the terrain is rendered as a 3D mesh. osm3d is used to download OSM data from a GeoJSON data structure in 3D, in other words, the coordinates of the OSM features contain not just latitude and longitude, but elevation in metres. The elevation is obtained from the DEM emitted from terrarium-dem , above; osm3d automatically listens for the terrarium-dem-loaded event emitted from terrarium-dem and uses the DEM to calculate the elevation. You can find out how these components work in more detail by visiting their GitHub repository .","title":"How the aframe-osm-3d components work"},{"location":"location-based-tutorial/#tiling","text":"The aframe-osm-3d components discussed above both work using a tiling system. Regions of the world are split up into tiles , using the \"XYZ\" or \"Google\" tiling system, which you can read about in more detail here . The general idea is that each tile is defined by an X, a Y, and a Z coordinate, in which: X represents the tile's coordinate on a west-east axis; Y represents the tile's coordinate on a north-south axis; Z represents the tile's zoom level, discussed in more detail in the link above. terrarium-dem reads in a given longitude and latitude, and downloads the current XYZ elevation tile at a given zoom level, and the eight surrounding tiles (nine in total). These tiles are internally cached by terrarium-dem so that if a nearby location is requested, and the user hasn't moved to a different tile, the cached data will be used, avoiding unnecessary downloads. Also, when tiles are downloaded, they are emitted within the terrarium-dem-loaded event, which the osm3d component responds to by downloading the corresponding tiles from an OSM web API, calculating the OSM data's elevation using the DEM, and again caching and emitting the result.","title":"Tiling"},{"location":"location-based-tutorial/#the-code","text":"With that in mind, here is our code which makes use of terrarium-dem and osm3d to download elevation and OSM data to create peaks with elevations. We will start with the HTML: !DOCTYPE html html head meta charset= utf-8 / title AR.js Peak Finder /title script src= https://aframe.io/releases/1.0.4/aframe.min.js /script script src= https://unpkg.com/aframe-look-at-component@0.8.0/dist/aframe-look-at-component.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script script src= js/bundle.js /script /head body a-scene vr-mode-ui= enabled: false arjs='sourceType: webcam; videoTexture: true; debugUIEnabled: false;' a-camera gps-projected-camera rotation-reader /a-camera a-entity terrarium-dem='zoom: 12; url: proxy.php?x={x} y={y} z={z}' osm3d='url: https://hikar.org/fm/ws/tsvr.php?x={x} y={y} z={z} amp;poi=natural amp;outProj=4326' peakfinder /a-entity /a-scene /body /html What's new here? Note that we are linking in our JavaScript (which we will come to shortly) as a bundle . This is a great way of developing code which relies on third-party modules without having to link multiple JS files, and we will expand upon this below. Note how our custom entity now has some additional components: the terrarium-dem and osm3d components which we discussed above. Note how each component has a url property defining the source URL to download the data. Note also how we set the zoom to 12 for terrarium-dem . This will use the zoom level of 12 for our tiles, which means that peaks several kilometres away in every direction will be downloaded on startup. You'll notice that the url property for terrarium-dem is not an AWS URL but rather a local URL, proxy.php . Why is this? Due to the same-origin-policy we cannot contact AWS directly via AJAX, so we need to create a proxy script to do it for us. The example provided is in PHP and will look something like the code below. (If your server does not have PHP installed, feel free to replace this code with another language). ?php $x = $_GET[ x ]; $y = $_GET[ y ]; $z = $_GET[ z ]; if(ctype_digit($x) ctype_digit($y) ctype_digit($z)) { header( Content-Type: image/png ); echo file_get_contents( https://s3.amazonaws.com/elevation-tiles-prod/terrarium/$z/$x/$y.png ); } else { header( HTTP/1.1 400 Bad Request ); header( Content-Type: application/json ); echo json_encode([ error = invalid x, y and/or z params ]); } ? For the OSM URL (on hikar.org ), we do not need a proxy as this script has CORS enabled by default. We can now move on to our JavaScript. This will look something like this: require('aframe-osm-3d'); AFRAME.registerComponent('peakfinder', { schema: { scale: { type: 'number', default: 15 } }, init: function() { this.textScale = this.data.scale * 100; this.camera = document.querySelector('a-camera'); window.addEventListener('gps-camera-update-position', e = { this.el.setAttribute('terrarium-dem', { lat: e.detail.position.latitude, lon: e.detail.position.longitude }) }); this.el.addEventListener('elevation-available', e = { const position = this.camera.getAttribute('position'); position.y = e.detail.elevation + 1.6; this.camera.setAttribute('position', position); }); this.el.addEventListener('osm-data-loaded', e = { e.detail.pois .filter ( f = f.properties.natural == 'peak' ) .forEach ( peak = { const entity = document.createElement('a-entity'); entity.setAttribute('look-at', '[gps-projected-camera]'); const text = document.createElement('a-text'); text.setAttribute('value', peak.properties.name); text.setAttribute('scale', { x: this.textScale, y: this.textScale, z: this.textScale }); text.setAttribute('align', 'center'); text.setAttribute('position', { x: 0, y: this.data.scale * 20, z: 0 }); entity.setAttribute('gps-projected-entity-place', { latitude: peak.geometry.coordinates[1], longitude: peak.geometry.coordinates[0] }); entity.setAttribute('position', { x: 0, y: peak.geometry.coordinates[2], z: 0 }); entity.appendChild(text); const cone = document.createElement('a-cone'); cone.setAttribute('radiusTop', 0.1); cone.setAttribute('scale', { x: this.data.scale * 10, y: this.data.scale * 10, z: this.data.scale * 10 }); cone.setAttribute('height', 3); cone.setAttribute('material', { color: 'magenta' } ); entity.appendChild(cone); this.el.appendChild(entity); }); }); } }); How does this work? We start by require ing the aframe-osm-3d package. This is a package available on NPM , and as we have seen, contains the terrarium-dem and osm3d components. aframe-osm-3d is a JavaScript module , specifically a CommonJS module, and this technique of require ing modules is standard in Node.js. However with the help of tools such as Browserify , we can use modules in the browser, too. More on this below. We then begin our actual component. Note how its schema includes a scale , allowing us to control the scale of the rendered peaks. Feel free to experiment with changing this. We then move on to our init() . This works a bit differently this time: We once again handle the gps-camera-update-position event, but this time we pass the latitude and longitude onto our terrarium-dem component. As we saw above, this will download the local tiles for this position. We next handle the elevation-available event, which is emitted from our terrarium-dem component as soon as we have elevation available for our current position. The elevation is contained in the emitted event; we handle it by setting the camera's y coordinate to that elevation plus 1.6 (to account for the fact that a user will be holding the device above the ground0. We then handle the osm-data-loaded event. This will be emitted from osm3d whenever new data has been downloaded from the OSM API and elevation added. This event contains various properties, but we will use the pois property which is an array of GeoJSON features, one for each POI. Note how we filter the POIs to select peaks only, once again. We then create one entity per peak, as before. We ensure the entity is looking at the camera, and then create two child entities, an a-cone primitive to show a peak-like graphic, and an a-text primitive to show the peak's name. We align the text to the centre so that it's correctly placed relative to the cone, and set coordinates and scale appropriately. We also add a gps-projected-entity-place component to each entity, as before, so that we can position them using their latitude and longitude. Note how, once we have set the latitude and longitude with gps-projected-entity-place , we set the elevation of each peak entity. We do this using the third member of the coordinates array contained within the GeoJSON, which will contain the peak's elevation in metres. And that is all that is needed to create a very simple peak finder in AR.js! We now need to build the peakfinder, as indicated above. First, we need to install the aframe-osm-3d package with NPM (included with Node.js ). npm install aframe-osm-3d Next, we need to create a bundle with Browserify . Browserify is an example of a \"bundling\" tool. It takes multiple JavaScript files, including third-party modules, and produces a single bundle , which can be linked to from an HTML page. To use, it's simply: browserify peakfinder.js bundle.js (assuming that you saved your peakfinder component as peakfinder.js ). And that is it! Try it out and see if you can see peaks in their correct location, and elevation.","title":"The code"},{"location":"location-based-tutorial/#further-things-to-try","text":"It would be nice to show users status messages as the peaks are downloading. You can do this by handling a couple of other events: terrarium-start-update is emitted when we start to download new DEM data. terrarium-dem-loaded , as we have already seen, is emitted when DEM data has finished downloading. Adapt your example to include a div which displays a status message. (You can make a div a child of your a-scene ). Display \"Downloading elevation data...\" while it's downloading the DEM data, and then \"Downloading OSM data\" when the DEM download has finished. Finally, when the OSM data has loaded, set the div 's contents to a blank string.","title":"Further things to try"},{"location":"location-based/","text":"Location Based Location Based has been implemented only for A-Frame framework. This article gives you a first glance to Location Based on AR.js. It can be used for indoor (but with low precision) and outdoor geopositioning of AR content. You can load places statically, from HTML or from Javascript, or you can load your data from local/remote json, or even through API calls. Choice is yours. On the article above there are all the options explained, as tutorials. Following there's the API Reference. gps-camera Required : yes Max allowed per scene : 1 This component enables the Location AR. It has to be added to the camera entity. It makes possible to handle both position and rotation of the camera and it's used to determine where the user is pointing their device. For example: a-camera gps-camera rotation-reader /a-camera In addition to that, as you can see on the example above, we also have to add rotation-reader to handle rotation events. See here for more details. Properties Property Description Default Value alert Whether to show a message when GPS signal is under the positionMinAccuracy false positionMinAccuracy Minimum accuracy allowed for position signal 100 minDistance If set, places with a distance from the user lower than this value, are not showed. Only a positive value is allowed. Value is in meters. 0 (disabled) maxDistance If set, places with a distance from the user higher than this value, are not showed. Only a positive value is allowed. Value is in meters. 0 (disabled) simulateLatitude Setting this allows you to simulate the latitude of the camera, to aid in testing. 0 (disabled) simulateLongitude Setting this allows you to simulate the longitude of the camera, to aid in testing. 0 (disabled) simulateAltitude Setting this allows you to simulate the altitude of the camera in meters above sea level, to aid in testing. 0 (disabled) gpsMinDistance Setting this allows you to control how far the camera must move, in meters, to generate a GPS update event. Useful to prevent 'jumping' of augmented content due to frequent small changes in position. 5 gpsTimeInterval Setting this allows you to control how frequently to obtain a new GPS position. If a previous GPS location is cached, the cached position will be used rather than a new position if its 'age' is less than this value, in milliseconds. This parameter is passed directly to the Geolocation API's watchPosition() method. 0 (always use new position, not cached) gps-entity-place Required : yes Max allowed per scene : no limit This component makes each entity GPS-trackable. This assigns a specific world position to an entity, so that the user can see it when their device is pointing to its position in the real world. If the user is far from the entity, it will seem smaller. If it's too far away, it won't be seen at all. It requires latitude and longitude as a single string parameter (example with a-box aframe primitive): a-box material= color: yellow gps-entity-place= latitude: your-latitude ; longitude: your-longitude / \u26a1\ufe0f In addition, you can use the a-frame \"position\" parameter to assign a y-value to change the height of the content. This value should be entered as meters above or below (if negative) the current camera height. For example, this would assign a height of 30 meters, and will be displayed relative to the gps-camera's current height: a-box material= color: yellow gps-entity-place= latitude: your-latitude ; longitude: your-longitude position= 0 30 0 / Properties No real property apart from the string that defined latitude and longitude together, as shown above. Custom Attributes The following are Custom Attributes that can be retrieved from gps-entity-place entities, for example: const distanceMsg = document.querySelector('[gps-entity-place]').getAttribute('distanceMsg'); console.log(distanceMsg); // 890 meters Custom Attribute Description Default Value distance Distance from user, updated at every user position update. Value in meters. 0 distanceMsg Distance from user as string, with unit, updated at every user position update. Value as distance meters/kilometers . '' Events Take a look at the UI and Events page for Location Based Custom Events. \u26a1\ufe0f Usually, in Location Based, it's nice to have the augmented content that will always face the user, so when you rotate the camera, 3D models or most of all, text, are well visible. Look at this example in order to create gps-entity-place entities that will always face the user (camera). Viewing every distant object If your location-based AR content is distant from the user (around 1km or more), it is recommended to use the new arjs-webcam-texture component (introduced in AR.js 3.2.0), which uses a THREE.js texture to stream the camera feed and allows distant content to be viewed. This component is automatically injected if the videoTexture parameter of the arjs system is set to true and the sourceType is webcam . For example (code snippet only): a-scene vr-mode-ui= enabled: false embedded arjs= sourceType: webcam; videoTexture: true; debugUIEnabled: false; Reducing shaking effects In location-based mode, 'shaking' effects can occur due to frequent small changes in the device's orientation, due to the high sensitivity of the device sensors such as the accelerometer. If using AR.js 3.3.1 or greater this can optionally be reduced using an exponential smoothing technique. Note that there are currently some occasional display artefacts with this if moving the device quickly or suddenly so please test before you enable it in a finished application; work to resolve these is on-going. This is enabled by adding the arjs-look-controls component to your a-camera with a smoothingFactor property. You must also disable A-Frame's default look-controls , as arjs-look-controls will replace it. For example: a-camera id='camera1' look-controls-enabled='false' arjs-look-controls='smoothingFactor: 0.1' gps-camera='gpsMinDistance: 5' rotation-reader /a-camera Exponential smoothing works by applying a smoothing factor to each newly-read device rotation angle (obtained from sensor readings) such that the previous smoothed value counts more than the current value, thus reducing 'noise' and 'jitter'. If k is the smoothing factor: smoothedAngle = k * newValue + (1 - k) * previousSmoothedAngle It can be seen from this that the smaller the value of k (the smoothingFactor property), the greater the smoothing effect. In tests, 0.1 appears to give the best result. Also you can reduce 'jumping' of augmented content when near a place - a bad-looking effect due to GPS sensors' low precision. To do so you can use the gpsMinDistance and gpsTimeInterval properties. See the Location Based specific docs to learn how to use them. Projected Camera Version The experimental 'projected camera' version of the location-based components for AR.js uses Spherical Mercator (aka EPSG:3857) to store both the camera position and the position of added points of interest and other geographical data. The rationale for this version is to allow easy addition of more complex geographic data such as roads and paths. Such data can be projected and added to an AR.js scene, and then, because Spherical Mercator units approximate to metres (away from the poles), the coordinates can be used directly as WebGL/A-Frame world coordinates. The two components for the projected camera version are gps-projected-camera and gps-projected-entity-place . Their interface is almost identical to gps-camera and gps-entity-place but they work differently internally. For example: a-camera gps-projected-camera rotation-reader /a-camera and: a-box color= yellow gps-projected-entity-place= latitude: your-latitude ; longitude: your-longitude / Note that internally, the latitude and longitude are converted to Spherical Mercator coordinates. As for gps-entity-place , you can specify an altitude using the y component of the position attribute: a-box color= yellow gps-projected-entity-place= latitude: your-latitude ; longitude: your-longitude position= 0 30 0 / Calculating world coordinates of arbitrary augmented content gps-projected-camera has some useful properties and methods which can be used to easily work with arbitrary augmented content (for example, polylines or polygons sourced from geodata APIs such as OpenStreetMap ). Before introducing these, it needs to be made clear that, in gps-projected-camera , the original GPS position is set as the world origin. So, if arbitrary content is to be added to the scene, and the source coordinates for this content is in unprojected (WGS84) latitude and longitude, it needs to be: projected to Spherical Mercator; and then converted to world coordinates relative to the original GPS position. On the other hand, only the second step is needed if the source coordinates are already projected. We'll look at each scenario now. Source data in WGS84 latitude/longitude This is probably the most common scenario. The latLonToWorld(lat, lon) method of the gps-projected-camera component converts latitude and longitude directly to world coordinates, performing the projection as the first step and then calculating the world coordinates from the projected coordinates. It will return a 2-member array containing the x and z world coordinates, allowing the developer to calculate or specify the y coordinate (altitude) independently. Source data in Spherical Mercator An alternative scenario is when the augmented content has already been projected into Spherical Mercator and therefore does not need the initial projection step when added to an AR.js scene. This may occur when an API serves data in Spherical Mercator, for instance. In this case, we still need to convert the Spherical Mercator coordinates to world coordinates relative to the original GPS position. gps-projected-camera has an originCoordsProjected property, which represents the original GPS position in Spherical Mercator coordinates. This is a two-member array, containing the Spherical Mercator easting and northing, respectively, of the origin point. From this, we can therefore work out the world coordinates from Spherical Mercator coordinates: xWorld = featureEasting - originCoordsProjected[0] and zWorld = -(featureNorthing - originCoordsProjected[1]) where xWorld and zWorld are the world x and z coordinates of the augmented content, and featureEasting and featureNorthing are the content's Spherical Mercator coordinates. Note how we have to reverse the sign of z as increasing Spherical Mercator easting corresponds to increasing x in OpenGL coordinates, and increasing altitude corresponds to increasing y , but increasing Spherical Mercator northing corresponds to decreasing z .","title":"Location Based"},{"location":"location-based/#location-based","text":"Location Based has been implemented only for A-Frame framework. This article gives you a first glance to Location Based on AR.js. It can be used for indoor (but with low precision) and outdoor geopositioning of AR content. You can load places statically, from HTML or from Javascript, or you can load your data from local/remote json, or even through API calls. Choice is yours. On the article above there are all the options explained, as tutorials. Following there's the API Reference.","title":"Location Based"},{"location":"location-based/#gps-camera","text":"Required : yes Max allowed per scene : 1 This component enables the Location AR. It has to be added to the camera entity. It makes possible to handle both position and rotation of the camera and it's used to determine where the user is pointing their device. For example: a-camera gps-camera rotation-reader /a-camera In addition to that, as you can see on the example above, we also have to add rotation-reader to handle rotation events. See here for more details.","title":"gps-camera"},{"location":"location-based/#properties","text":"Property Description Default Value alert Whether to show a message when GPS signal is under the positionMinAccuracy false positionMinAccuracy Minimum accuracy allowed for position signal 100 minDistance If set, places with a distance from the user lower than this value, are not showed. Only a positive value is allowed. Value is in meters. 0 (disabled) maxDistance If set, places with a distance from the user higher than this value, are not showed. Only a positive value is allowed. Value is in meters. 0 (disabled) simulateLatitude Setting this allows you to simulate the latitude of the camera, to aid in testing. 0 (disabled) simulateLongitude Setting this allows you to simulate the longitude of the camera, to aid in testing. 0 (disabled) simulateAltitude Setting this allows you to simulate the altitude of the camera in meters above sea level, to aid in testing. 0 (disabled) gpsMinDistance Setting this allows you to control how far the camera must move, in meters, to generate a GPS update event. Useful to prevent 'jumping' of augmented content due to frequent small changes in position. 5 gpsTimeInterval Setting this allows you to control how frequently to obtain a new GPS position. If a previous GPS location is cached, the cached position will be used rather than a new position if its 'age' is less than this value, in milliseconds. This parameter is passed directly to the Geolocation API's watchPosition() method. 0 (always use new position, not cached)","title":"Properties"},{"location":"location-based/#gps-entity-place","text":"Required : yes Max allowed per scene : no limit This component makes each entity GPS-trackable. This assigns a specific world position to an entity, so that the user can see it when their device is pointing to its position in the real world. If the user is far from the entity, it will seem smaller. If it's too far away, it won't be seen at all. It requires latitude and longitude as a single string parameter (example with a-box aframe primitive): a-box material= color: yellow gps-entity-place= latitude: your-latitude ; longitude: your-longitude / \u26a1\ufe0f In addition, you can use the a-frame \"position\" parameter to assign a y-value to change the height of the content. This value should be entered as meters above or below (if negative) the current camera height. For example, this would assign a height of 30 meters, and will be displayed relative to the gps-camera's current height: a-box material= color: yellow gps-entity-place= latitude: your-latitude ; longitude: your-longitude position= 0 30 0 /","title":"gps-entity-place"},{"location":"location-based/#properties_1","text":"No real property apart from the string that defined latitude and longitude together, as shown above.","title":"Properties"},{"location":"location-based/#custom-attributes","text":"The following are Custom Attributes that can be retrieved from gps-entity-place entities, for example: const distanceMsg = document.querySelector('[gps-entity-place]').getAttribute('distanceMsg'); console.log(distanceMsg); // 890 meters Custom Attribute Description Default Value distance Distance from user, updated at every user position update. Value in meters. 0 distanceMsg Distance from user as string, with unit, updated at every user position update. Value as distance meters/kilometers . ''","title":"Custom Attributes"},{"location":"location-based/#events","text":"Take a look at the UI and Events page for Location Based Custom Events. \u26a1\ufe0f Usually, in Location Based, it's nice to have the augmented content that will always face the user, so when you rotate the camera, 3D models or most of all, text, are well visible. Look at this example in order to create gps-entity-place entities that will always face the user (camera).","title":"Events"},{"location":"location-based/#viewing-every-distant-object","text":"If your location-based AR content is distant from the user (around 1km or more), it is recommended to use the new arjs-webcam-texture component (introduced in AR.js 3.2.0), which uses a THREE.js texture to stream the camera feed and allows distant content to be viewed. This component is automatically injected if the videoTexture parameter of the arjs system is set to true and the sourceType is webcam . For example (code snippet only): a-scene vr-mode-ui= enabled: false embedded arjs= sourceType: webcam; videoTexture: true; debugUIEnabled: false;","title":"Viewing every distant object"},{"location":"location-based/#reducing-shaking-effects","text":"In location-based mode, 'shaking' effects can occur due to frequent small changes in the device's orientation, due to the high sensitivity of the device sensors such as the accelerometer. If using AR.js 3.3.1 or greater this can optionally be reduced using an exponential smoothing technique. Note that there are currently some occasional display artefacts with this if moving the device quickly or suddenly so please test before you enable it in a finished application; work to resolve these is on-going. This is enabled by adding the arjs-look-controls component to your a-camera with a smoothingFactor property. You must also disable A-Frame's default look-controls , as arjs-look-controls will replace it. For example: a-camera id='camera1' look-controls-enabled='false' arjs-look-controls='smoothingFactor: 0.1' gps-camera='gpsMinDistance: 5' rotation-reader /a-camera Exponential smoothing works by applying a smoothing factor to each newly-read device rotation angle (obtained from sensor readings) such that the previous smoothed value counts more than the current value, thus reducing 'noise' and 'jitter'. If k is the smoothing factor: smoothedAngle = k * newValue + (1 - k) * previousSmoothedAngle It can be seen from this that the smaller the value of k (the smoothingFactor property), the greater the smoothing effect. In tests, 0.1 appears to give the best result. Also you can reduce 'jumping' of augmented content when near a place - a bad-looking effect due to GPS sensors' low precision. To do so you can use the gpsMinDistance and gpsTimeInterval properties. See the Location Based specific docs to learn how to use them.","title":"Reducing shaking effects"},{"location":"location-based/#projected-camera-version","text":"The experimental 'projected camera' version of the location-based components for AR.js uses Spherical Mercator (aka EPSG:3857) to store both the camera position and the position of added points of interest and other geographical data. The rationale for this version is to allow easy addition of more complex geographic data such as roads and paths. Such data can be projected and added to an AR.js scene, and then, because Spherical Mercator units approximate to metres (away from the poles), the coordinates can be used directly as WebGL/A-Frame world coordinates. The two components for the projected camera version are gps-projected-camera and gps-projected-entity-place . Their interface is almost identical to gps-camera and gps-entity-place but they work differently internally. For example: a-camera gps-projected-camera rotation-reader /a-camera and: a-box color= yellow gps-projected-entity-place= latitude: your-latitude ; longitude: your-longitude / Note that internally, the latitude and longitude are converted to Spherical Mercator coordinates. As for gps-entity-place , you can specify an altitude using the y component of the position attribute: a-box color= yellow gps-projected-entity-place= latitude: your-latitude ; longitude: your-longitude position= 0 30 0 /","title":"Projected Camera Version"},{"location":"location-based/#calculating-world-coordinates-of-arbitrary-augmented-content","text":"gps-projected-camera has some useful properties and methods which can be used to easily work with arbitrary augmented content (for example, polylines or polygons sourced from geodata APIs such as OpenStreetMap ). Before introducing these, it needs to be made clear that, in gps-projected-camera , the original GPS position is set as the world origin. So, if arbitrary content is to be added to the scene, and the source coordinates for this content is in unprojected (WGS84) latitude and longitude, it needs to be: projected to Spherical Mercator; and then converted to world coordinates relative to the original GPS position. On the other hand, only the second step is needed if the source coordinates are already projected. We'll look at each scenario now.","title":"Calculating world coordinates of arbitrary augmented content"},{"location":"location-based/#source-data-in-wgs84-latitudelongitude","text":"This is probably the most common scenario. The latLonToWorld(lat, lon) method of the gps-projected-camera component converts latitude and longitude directly to world coordinates, performing the projection as the first step and then calculating the world coordinates from the projected coordinates. It will return a 2-member array containing the x and z world coordinates, allowing the developer to calculate or specify the y coordinate (altitude) independently.","title":"Source data in WGS84 latitude/longitude"},{"location":"location-based/#source-data-in-spherical-mercator","text":"An alternative scenario is when the augmented content has already been projected into Spherical Mercator and therefore does not need the initial projection step when added to an AR.js scene. This may occur when an API serves data in Spherical Mercator, for instance. In this case, we still need to convert the Spherical Mercator coordinates to world coordinates relative to the original GPS position. gps-projected-camera has an originCoordsProjected property, which represents the original GPS position in Spherical Mercator coordinates. This is a two-member array, containing the Spherical Mercator easting and northing, respectively, of the origin point. From this, we can therefore work out the world coordinates from Spherical Mercator coordinates: xWorld = featureEasting - originCoordsProjected[0] and zWorld = -(featureNorthing - originCoordsProjected[1]) where xWorld and zWorld are the world x and z coordinates of the augmented content, and featureEasting and featureNorthing are the content's Spherical Mercator coordinates. Note how we have to reverse the sign of z as increasing Spherical Mercator easting corresponds to increasing x in OpenGL coordinates, and increasing altitude corresponds to increasing y , but increasing Spherical Mercator northing corresponds to decreasing z .","title":"Source data in Spherical Mercator"},{"location":"marker-based/","text":"Marker Based Markers can be of three, different types: Hiro Barcode Pattern. To learn more about markers, please read this articles: AR.js basic Marker Based tutorial and Markers explanation Deliver AR.js experiences using only QRCodes (Markers inside QRCodes) . TL:DR Hiro Marker is the default one, not very useful actually Barcode markers are auto-generated markers, from matrix computations. Learn more on the above articles on how to use them. If you need the full list of barcode markers, here it is Pattern markers are custom ones, created starting from an image (very simple, hight contrast), loaded by the user. \u26a1\ufe0f You can create your Pattern Markers with this tool . It will generate an image to scan and a .patt file, to be loaded on the AR.js web app, in order for it to recognise the marker when running. How to choose good images for Pattern Markers Markers have a black border and high contrast shapes. Lately, we have added also white border markers with black background, altough the classic ones, with black border, behave better. Here's an article explaining all good practice on how to choose good images to be used to generate custom markers: 10 tips to enhance your AR.js app . API Reference for Marker Based A-Frame a-marker/ Here are the attributes for this entity Attribute Description Component Mapping type type of marker - ['pattern', 'barcode', 'unknown' ] artoolkitmarker.type size size of the marker in meter artoolkitmarker.size url url of the pattern - IIF type='pattern' artoolkitmarker.patternUrl value value of the barcode - IIF type='barcode' artoolkitmarker.barcodeValue preset parameters preset - ['hiro', 'kanji'] artoolkitmarker.preset emitevents emits 'markerFound' and 'markerLost' events - ['true', 'false'] - smooth turn on/off camera smoothing - ['true', 'false'] - default: false - smoothCount number of matrices to smooth tracking over, more = smoother but slower follow - default: 5 - smoothTolerance distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still - default: 0.01 - smoothThreshold threshold for smoothing, will keep still unless enough matrices are over tolerance - default: 2 - three.js threex-artoolkit threex.artoolkit is the three.js extension to easily handle artoolkit . Architecture threex.artoolkit is composed of 3 classes THREEx.ArToolkitSource : It is the image which is analyzed to do the position tracking. It can be the webcam, a video or even an image THREEx.ArToolkitContext : It is the main engine. It will actually find the marker position in the image source. THREEx.ArMarkerControls : it controls the position of the marker It use the classical three.js controls API . It will make sure to position your content right on top of the marker. THREEx.ArMarkerControls var parameters = { // size of the marker in meter size: 1, // type of marker - ['pattern', 'barcode', 'unknown' ] type: unknown , // url of the pattern - IIF type='pattern' patternUrl: null, // value of the barcode - IIF type='barcode' barcodeValue: null, // change matrix mode - [modelViewMatrix, cameraTransformMatrix] changeMatrixMode: modelViewMatrix , // turn on/off camera smoothing smooth: true, // number of matrices to smooth tracking over, more = smoother but slower follow smoothCount: 5, // distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still smoothTolerance: 0.01, // threshold for smoothing, will keep still unless enough matrices are over tolerance smoothThreshold: 2 }; THREEx.ArToolkitContext var parameters = { // debug - true if one should display artoolkit debug canvas, false otherwise debug: false, // the mode of detection - ['color', 'color_and_matrix', 'mono', 'mono_and_matrix'] detectionMode: 'color_and_matrix', // type of matrix code - valid iif detectionMode end with 'matrix' - [3x3, 3x3_HAMMING63, 3x3_PARITY65, 4x4, 4x4_BCH_13_9_3, 4x4_BCH_13_5_5] matrixCodeType: '3x3', // Pattern ratio for custom markers patternRatio: 0.5 // Labeling mode for markers - ['black_region', 'white_region'] // black_region: Black bordered markers on a white background, white_region: White bordered markers on a black background labelingMode: 'black_region', // url of the camera parameters cameraParametersUrl: THREEx.ArToolkitContext.baseURL + '../data/data/camera_para.dat', // tune the maximum rate of pose detection in the source image maxDetectionRate: 60, // resolution of at which we detect pose in the source image canvasWidth: 640, canvasHeight: 480, // enable image smoothing or not for canvas copy - default to true // https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/imageSmoothingEnabled imageSmoothingEnabled : true, } THREEx.ArToolkitSource var parameters = { // type of source - ['webcam', 'image', 'video'] sourceType: webcam , // url of the source - valid if sourceType = image|video sourceUrl: null, // resolution of at which we initialize the source image sourceWidth: 640, sourceHeight: 480, // resolution displayed for the source displayWidth: 640, displayHeight: 480 };","title":"Marker Based"},{"location":"marker-based/#marker-based","text":"Markers can be of three, different types: Hiro Barcode Pattern. To learn more about markers, please read this articles: AR.js basic Marker Based tutorial and Markers explanation Deliver AR.js experiences using only QRCodes (Markers inside QRCodes) . TL:DR Hiro Marker is the default one, not very useful actually Barcode markers are auto-generated markers, from matrix computations. Learn more on the above articles on how to use them. If you need the full list of barcode markers, here it is Pattern markers are custom ones, created starting from an image (very simple, hight contrast), loaded by the user. \u26a1\ufe0f You can create your Pattern Markers with this tool . It will generate an image to scan and a .patt file, to be loaded on the AR.js web app, in order for it to recognise the marker when running.","title":"Marker Based"},{"location":"marker-based/#how-to-choose-good-images-for-pattern-markers","text":"Markers have a black border and high contrast shapes. Lately, we have added also white border markers with black background, altough the classic ones, with black border, behave better. Here's an article explaining all good practice on how to choose good images to be used to generate custom markers: 10 tips to enhance your AR.js app .","title":"How to choose good images for Pattern Markers"},{"location":"marker-based/#api-reference-for-marker-based","text":"","title":"API Reference for Marker Based"},{"location":"marker-based/#a-frame","text":"","title":"A-Frame"},{"location":"marker-based/#lta-markergt","text":"Here are the attributes for this entity Attribute Description Component Mapping type type of marker - ['pattern', 'barcode', 'unknown' ] artoolkitmarker.type size size of the marker in meter artoolkitmarker.size url url of the pattern - IIF type='pattern' artoolkitmarker.patternUrl value value of the barcode - IIF type='barcode' artoolkitmarker.barcodeValue preset parameters preset - ['hiro', 'kanji'] artoolkitmarker.preset emitevents emits 'markerFound' and 'markerLost' events - ['true', 'false'] - smooth turn on/off camera smoothing - ['true', 'false'] - default: false - smoothCount number of matrices to smooth tracking over, more = smoother but slower follow - default: 5 - smoothTolerance distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still - default: 0.01 - smoothThreshold threshold for smoothing, will keep still unless enough matrices are over tolerance - default: 2 -","title":"&lt;a-marker/&gt;"},{"location":"marker-based/#threejs","text":"","title":"three.js"},{"location":"marker-based/#threex-artoolkit","text":"threex.artoolkit is the three.js extension to easily handle artoolkit .","title":"threex-artoolkit"},{"location":"marker-based/#architecture","text":"threex.artoolkit is composed of 3 classes THREEx.ArToolkitSource : It is the image which is analyzed to do the position tracking. It can be the webcam, a video or even an image THREEx.ArToolkitContext : It is the main engine. It will actually find the marker position in the image source. THREEx.ArMarkerControls : it controls the position of the marker It use the classical three.js controls API . It will make sure to position your content right on top of the marker.","title":"Architecture"},{"location":"marker-based/#threexarmarkercontrols","text":"var parameters = { // size of the marker in meter size: 1, // type of marker - ['pattern', 'barcode', 'unknown' ] type: unknown , // url of the pattern - IIF type='pattern' patternUrl: null, // value of the barcode - IIF type='barcode' barcodeValue: null, // change matrix mode - [modelViewMatrix, cameraTransformMatrix] changeMatrixMode: modelViewMatrix , // turn on/off camera smoothing smooth: true, // number of matrices to smooth tracking over, more = smoother but slower follow smoothCount: 5, // distance tolerance for smoothing, if smoothThreshold # of matrices are under tolerance, tracking will stay still smoothTolerance: 0.01, // threshold for smoothing, will keep still unless enough matrices are over tolerance smoothThreshold: 2 };","title":"THREEx.ArMarkerControls"},{"location":"marker-based/#threexartoolkitcontext","text":"var parameters = { // debug - true if one should display artoolkit debug canvas, false otherwise debug: false, // the mode of detection - ['color', 'color_and_matrix', 'mono', 'mono_and_matrix'] detectionMode: 'color_and_matrix', // type of matrix code - valid iif detectionMode end with 'matrix' - [3x3, 3x3_HAMMING63, 3x3_PARITY65, 4x4, 4x4_BCH_13_9_3, 4x4_BCH_13_5_5] matrixCodeType: '3x3', // Pattern ratio for custom markers patternRatio: 0.5 // Labeling mode for markers - ['black_region', 'white_region'] // black_region: Black bordered markers on a white background, white_region: White bordered markers on a black background labelingMode: 'black_region', // url of the camera parameters cameraParametersUrl: THREEx.ArToolkitContext.baseURL + '../data/data/camera_para.dat', // tune the maximum rate of pose detection in the source image maxDetectionRate: 60, // resolution of at which we detect pose in the source image canvasWidth: 640, canvasHeight: 480, // enable image smoothing or not for canvas copy - default to true // https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/imageSmoothingEnabled imageSmoothingEnabled : true, }","title":"THREEx.ArToolkitContext"},{"location":"marker-based/#threexartoolkitsource","text":"var parameters = { // type of source - ['webcam', 'image', 'video'] sourceType: webcam , // url of the source - valid if sourceType = image|video sourceUrl: null, // resolution of at which we initialize the source image sourceWidth: 640, sourceHeight: 480, // resolution displayed for the source displayWidth: 640, displayHeight: 480 };","title":"THREEx.ArToolkitSource"},{"location":"ui-events/","text":"UI and Custom Events To make AR.js based Web App looking better and add UI capabilities, it's possible to treat is as common website. Here you will learn how to use Raycaster, Custom Events and Interaction with overlayed DOM elements. Handle clicks on AR content It's now possible to use AR.js (marker based or image tracking) with a-frame latest versions (1.0.0 and above) in order to have touch gestures to zoom and rotate your content! Disclaimer: this will work for your entire a-scene , so it's not a real option if you have to handle different interactions for multiple markers. It will work like charm if you have one marker/image for scene. Check Fabio Cort\u00e8s great walkthrough in order to add this feature on your AR.js web app. You can use this exact approach for Image Tracking a-nft and Marker Based a-entity elements. The clickhandler name can be customized, you can choose the one you like most, it's just a reference. Keep in mind that this click/touch interaction is not handled by AR.js at all, it is all A-Frame based. Always look on the A-Frame documentation for more details. Check out the tutorial Interaction with Overlayed DOM content You can add interations by adding DOM HTML elements on the body . For example, starting from this example: !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script body style= margin : 0px; overflow: hidden; a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html We can add on the body, outside the a-scene : div class= buttons button class= say-hi-button /button /div Then, we need to add some CSS to absolute positioning the DIV and BUTTON, and also some scripting to listen to click events. You can customize your a-scene or content, like 3D models, play video, and so on. See on A-Frame Docs on how to change entity properties and work with events: https://aframe.io/docs/1.0.0/introduction/javascript-events-dom-apis.html. We will end up with the following code: !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script script window.onload = function () { document .querySelector( .say-hi-button ) .addEventListener( click , function () { // here you can change also a-scene or a-entity properties, like // changing your 3D model source, size, position and so on // or you can just open links, trigger actions... alert( Hi there! ); }); }; /script style .buttons { position: absolute; bottom: 0; left: 0; width: 100%; height: 5em; display: flex; justify-content: center; align-items: center; z-index: 10; } .say-hi-button { padding: 0.25em; border-radius: 4px; border: none; background: white; color: black; width: 4em; height: 2em; } /style body style= margin : 0px; overflow: hidden; div class= buttons button class= say-hi-button SAY HI! /button /div a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html Custom Events AR.js dispatches several Custom Events. Some of them are general, others are specific for AR Feature. Here's the full list. Custom Event name Description Payload Source File Feature arjs-video-loaded Fired when camera video stream has been appended to the DOM { detail: { component: HTMLElement }} threex-artoolkitsource.js all camera-error Fired when camera video stream could not be retrieved { error: Error } threex-artoolkitsource.js all camera-init Fired when camera video stream has been retrieved correctly { stream: MediaStream } threex-artoolkitsource.js all markerFound Fired when a marker in Marker Based, or a picture in Image Tracking, has been found - component-anchor.js only Image Tracking and Marker Based markerLost Fired when a marker in Marker Based, or a picture in Image Tracking, has been lost - component-anchor.js only Image Tracking and Marker Based arjs-nft-loaded Fired when a nft marker is full loaded threex-armarkercontrols-nft-start.js only Image Tracking gps-camera-update-positon Fired when gps-camera has updated its position { detail: { position: GeolocationCoordinates , origin: GeolocationCoordinates }} gps-camera.js only Location Based gps-entity-place-update-positon Fired when gps-entity-place has updated its position { detail: { distance: Number }} gps-entity-place.js only Location Based gps-entity-place-added Fired when the gps-entity-place has been added { detail: { component: HTMLElement }} gps-entity-place.js only Location Based gps-camera-origin-coord-set Fired when the origin coordinates are set - gps-camera.js only Location Based gps-entity-place-loaded Fired when the gps-entity-place has been - see 'loaded' event of A-Frame entities { detail: { component: HTMLElement }} gps-entity-place.js only Location Based Internal Loading Events \u26a1\ufe0f Both Image Tracking and Location Based automatically handle an internal event when origin location has been set Image Tracking (Image Descriptors) are fully loaded And automatically remove from the DOM elements that match the .arjs-loader selector. You can add any custom loader that will be remove in the above situations, just use the .arjs-loader class on it. Trigger actions when image has been found You can trigger any action you want when marker/image has been found. You can avoid linking a content to a marker/image and only trigger an action (like a redirect to an external website) when the anchor has been found by the camera. script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style script AFRAME.registerComponent('markerhandler', { init: function () { this.el.sceneEl.addEventListener('markerFound', () = { // redirect to custom URL window.location = 'https://github.com/AR-js-org/AR.js'; }); }); }, /script body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded -- div class= arjs-loader div Loading, please wait... /div /div a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- we use cors proxy to avoid cross-origin problems -- !-- we use the trex image shown on the homepage of the docs -- a-nft markerhandler type= nft url= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/trex-image/trex /a-nft a-entity camera /a-entity /a-scene /body Trigger action when marker has been found script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script script AFRAME.registerComponent('markerhandler', { init: function () { this.el.sceneEl.addEventListener('markerFound', () = { // redirect to custom URL e.g. google.com window.location = 'https://www.google.com/'; }) } }); /script body style= margin : 0px; overflow: hidden; a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam; debugUIEnabled: false; detectionMode: mono_and_matrix; matrixCodeType: 3x3; a-marker markerhandler type='barcode' value='7' a-box position='0 0.5 0' color= yellow /a-box /a-marker a-entity camera /a-entity /a-scene /body","title":"UI and Events"},{"location":"ui-events/#ui-and-custom-events","text":"To make AR.js based Web App looking better and add UI capabilities, it's possible to treat is as common website. Here you will learn how to use Raycaster, Custom Events and Interaction with overlayed DOM elements.","title":"UI and Custom Events"},{"location":"ui-events/#handle-clicks-on-ar-content","text":"It's now possible to use AR.js (marker based or image tracking) with a-frame latest versions (1.0.0 and above) in order to have touch gestures to zoom and rotate your content! Disclaimer: this will work for your entire a-scene , so it's not a real option if you have to handle different interactions for multiple markers. It will work like charm if you have one marker/image for scene. Check Fabio Cort\u00e8s great walkthrough in order to add this feature on your AR.js web app. You can use this exact approach for Image Tracking a-nft and Marker Based a-entity elements. The clickhandler name can be customized, you can choose the one you like most, it's just a reference. Keep in mind that this click/touch interaction is not handled by AR.js at all, it is all A-Frame based. Always look on the A-Frame documentation for more details. Check out the tutorial","title":"Handle clicks on AR content"},{"location":"ui-events/#interaction-with-overlayed-dom-content","text":"You can add interations by adding DOM HTML elements on the body . For example, starting from this example: !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script body style= margin : 0px; overflow: hidden; a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html We can add on the body, outside the a-scene : div class= buttons button class= say-hi-button /button /div Then, we need to add some CSS to absolute positioning the DIV and BUTTON, and also some scripting to listen to click events. You can customize your a-scene or content, like 3D models, play video, and so on. See on A-Frame Docs on how to change entity properties and work with events: https://aframe.io/docs/1.0.0/introduction/javascript-events-dom-apis.html. We will end up with the following code: !DOCTYPE html html script src= https://aframe.io/releases/1.0.4/aframe.min.js /script !-- we import arjs version without NFT but with marker + location based support -- script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar.js /script script window.onload = function () { document .querySelector( .say-hi-button ) .addEventListener( click , function () { // here you can change also a-scene or a-entity properties, like // changing your 3D model source, size, position and so on // or you can just open links, trigger actions... alert( Hi there! ); }); }; /script style .buttons { position: absolute; bottom: 0; left: 0; width: 100%; height: 5em; display: flex; justify-content: center; align-items: center; z-index: 10; } .say-hi-button { padding: 0.25em; border-radius: 4px; border: none; background: white; color: black; width: 4em; height: 2em; } /style body style= margin : 0px; overflow: hidden; div class= buttons button class= say-hi-button SAY HI! /button /div a-scene embedded arjs a-marker preset= hiro a-entity position= 0 0 0 scale= 0.05 0.05 0.05 gltf-model= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/scene.gltf /a-entity /a-marker a-entity camera /a-entity /a-scene /body /html","title":"Interaction with Overlayed DOM content"},{"location":"ui-events/#custom-events","text":"AR.js dispatches several Custom Events. Some of them are general, others are specific for AR Feature. Here's the full list. Custom Event name Description Payload Source File Feature arjs-video-loaded Fired when camera video stream has been appended to the DOM { detail: { component: HTMLElement }} threex-artoolkitsource.js all camera-error Fired when camera video stream could not be retrieved { error: Error } threex-artoolkitsource.js all camera-init Fired when camera video stream has been retrieved correctly { stream: MediaStream } threex-artoolkitsource.js all markerFound Fired when a marker in Marker Based, or a picture in Image Tracking, has been found - component-anchor.js only Image Tracking and Marker Based markerLost Fired when a marker in Marker Based, or a picture in Image Tracking, has been lost - component-anchor.js only Image Tracking and Marker Based arjs-nft-loaded Fired when a nft marker is full loaded threex-armarkercontrols-nft-start.js only Image Tracking gps-camera-update-positon Fired when gps-camera has updated its position { detail: { position: GeolocationCoordinates , origin: GeolocationCoordinates }} gps-camera.js only Location Based gps-entity-place-update-positon Fired when gps-entity-place has updated its position { detail: { distance: Number }} gps-entity-place.js only Location Based gps-entity-place-added Fired when the gps-entity-place has been added { detail: { component: HTMLElement }} gps-entity-place.js only Location Based gps-camera-origin-coord-set Fired when the origin coordinates are set - gps-camera.js only Location Based gps-entity-place-loaded Fired when the gps-entity-place has been - see 'loaded' event of A-Frame entities { detail: { component: HTMLElement }} gps-entity-place.js only Location Based","title":"Custom Events"},{"location":"ui-events/#internal-loading-events","text":"\u26a1\ufe0f Both Image Tracking and Location Based automatically handle an internal event when origin location has been set Image Tracking (Image Descriptors) are fully loaded And automatically remove from the DOM elements that match the .arjs-loader selector. You can add any custom loader that will be remove in the above situations, just use the .arjs-loader class on it.","title":"Internal Loading Events"},{"location":"ui-events/#trigger-actions-when-image-has-been-found","text":"You can trigger any action you want when marker/image has been found. You can avoid linking a content to a marker/image and only trigger an action (like a redirect to an external website) when the anchor has been found by the camera. script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script style .arjs-loader { height: 100%; width: 100%; position: absolute; top: 0; left: 0; background-color: rgba(0, 0, 0, 0.8); z-index: 9999; display: flex; justify-content: center; align-items: center; } .arjs-loader div { text-align: center; font-size: 1.25em; color: white; } /style script AFRAME.registerComponent('markerhandler', { init: function () { this.el.sceneEl.addEventListener('markerFound', () = { // redirect to custom URL window.location = 'https://github.com/AR-js-org/AR.js'; }); }); }, /script body style= margin : 0px; overflow: hidden; !-- minimal loader shown until image descriptors are loaded -- div class= arjs-loader div Loading, please wait... /div /div a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam;debugUIEnabled: false; !-- we use cors proxy to avoid cross-origin problems -- !-- we use the trex image shown on the homepage of the docs -- a-nft markerhandler type= nft url= https://arjs-cors-proxy.herokuapp.com/https://raw.githack.com/AR-js-org/AR.js/master/aframe/examples/image-tracking/nft/trex/trex-image/trex /a-nft a-entity camera /a-entity /a-scene /body","title":"Trigger actions when image has been found"},{"location":"ui-events/#trigger-action-when-marker-has-been-found","text":"script src= https://cdn.jsdelivr.net/gh/aframevr/aframe@1c2407b26c61958baa93967b5412487cd94b290b/dist/aframe-master.min.js /script script src= https://raw.githack.com/AR-js-org/AR.js/master/aframe/build/aframe-ar-nft.js /script script AFRAME.registerComponent('markerhandler', { init: function () { this.el.sceneEl.addEventListener('markerFound', () = { // redirect to custom URL e.g. google.com window.location = 'https://www.google.com/'; }) } }); /script body style= margin : 0px; overflow: hidden; a-scene vr-mode-ui= enabled: false; renderer= logarithmicDepthBuffer: true; embedded arjs= trackingMethod: best; sourceType: webcam; debugUIEnabled: false; detectionMode: mono_and_matrix; matrixCodeType: 3x3; a-marker markerhandler type='barcode' value='7' a-box position='0 0.5 0' color= yellow /a-box /a-marker a-entity camera /a-entity /a-scene /body","title":"Trigger action when marker has been found"}]}